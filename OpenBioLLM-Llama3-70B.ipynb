{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importing Required Libraries\n",
    "import transformers  # Hugging Face transformers library for working with pre-trained models\n",
    "import torch # PyTorch library for deep learning operations\n",
    "\n",
    "# 2. Model Configuration\n",
    "model_id = \"aaditya/OpenBioLLM-Llama3-70B\" # Specifies which model to use from HuggingFace\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",        # Task type\n",
    "    model=model_id,          # Model identifier\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16  # Uses bfloat16 precision for memory efficiency\n",
    "    },\n",
    "    device=\"auto\",           # Automatically selects best available device (CPU/GPU)\n",
    ")\n",
    "\n",
    "# 3. Setting up the Chat Template\n",
    "messages = [\n",
    "    # System prompt that defines the AI's behavior\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Your name is OpenBioLLM, and you were developed by Saama AI Labs. who's willing to help answer the user's query with explanation. In your explanation, leverage your deep medical expertise such as relevant anatomical structures, physiological processes, diagnostic criteria, treatment guidelines, or other pertinent medical concepts. Use precise medical terminology while still aiming to make the explanation clear and accessible to a general audience.\"},\n",
    "    # User's input\n",
    "    {\"role\": \"user\", \"content\": \"How can i split a 3mg or 4mg waefin pill so i can get a 2.5mg pill?\"},\n",
    "]\n",
    "\n",
    "# 4. Formatting the Prompt\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,          # Returns string instead of tokens\n",
    "    add_generation_prompt=True  # Adds any necessary generation markers\n",
    ")\n",
    "\n",
    "# 5. Setting up Response Termination Conditions\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,  # End of sequence token\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  # Custom end token\n",
    "]\n",
    "\n",
    "# 6. Generating Response\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,      # Maximum length of generated response\n",
    "    eos_token_id=terminators,  # When to stop generating\n",
    "    do_sample=True,          # Enable sampling-based generation\n",
    "    temperature=0.0,         # Controls randomness (0.0 = deterministic)\n",
    "    top_p=0.9,              # Nucleus sampling parameter\n",
    ")\n",
    "\n",
    "# 7. Extracting and Printing Response\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

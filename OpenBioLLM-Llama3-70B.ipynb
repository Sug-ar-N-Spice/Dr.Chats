{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for machine learning and deep learning\n",
    "import transformers  # Hugging Face library for working with pre-trained models\n",
    "import torch        # PyTorch library for tensor computations and neural networks\n",
    "import typing       # Library for type hinting and type annotations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalChatbot:\n",
    "    # Dictionary mapping medical question categories to tailored system prompts\n",
    "    category_prompts = {\n",
    "        \"Basic Medical Knowledge\": (\n",
    "            \"You are an experienced medical professional specializing in general medical knowledge. \"\n",
    "            \"Provide detailed and comprehensible answers to basic medical questions, ensuring accuracy and clarity.\"\n",
    "        ),\n",
    "        \"Pharmacological Queries\": (\n",
    "            \"You are a pharmacology expert with extensive knowledge of drugs, their mechanisms, and side effects. \"\n",
    "            \"Provide precise and evidence-based information about medications and treatments.\"\n",
    "        ),\n",
    "        \"Diagnostic Reasoning\": (\n",
    "            \"You are a diagnostic expert proficient in identifying medical conditions. \"\n",
    "            \"Offer clear and methodical guidance for diagnostic reasoning and differential diagnoses.\"\n",
    "        ),\n",
    "        \"Treatment & Management\": (\n",
    "            \"You are a specialist in medical treatment and patient management strategies. \"\n",
    "            \"Provide comprehensive explanations of treatment guidelines and management protocols.\"\n",
    "        ),\n",
    "        \"Specialized Medical Topics\": (\n",
    "            \"You are an expert in advanced medical research and innovations. \"\n",
    "            \"Discuss specialized medical topics with a focus on the latest scientific advances.\"\n",
    "        ),\n",
    "        \"Preventive Medicine\": (\n",
    "            \"You are a preventive medicine specialist with expertise in lifestyle modifications and public health. \"\n",
    "            \"Explain preventive strategies and their importance in reducing disease risk.\"\n",
    "        ),\n",
    "        \"Specialized Medical Scenarios\": (\n",
    "            \"You are a clinical specialist adept at handling unique and complex medical scenarios. \"\n",
    "            \"Provide nuanced insights into complications and specialized conditions.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Fallback system prompt for undefined categories\n",
    "    default_prompt = (\n",
    "        \"You are an expert and experienced medical professional with extensive medical knowledge. \"\n",
    "        \"Provide precise, evidence-based medical explanations that are scientifically accurate and comprehensible to a general audience.\"\n",
    "    )\n",
    "\n",
    "    def __init__(self, model_id=\"aaditya/OpenBioLLM-Llama3-70B\"):\n",
    "        \"\"\"\n",
    "        Initialize the medical chatbot with a specific medical language model\n",
    "        \n",
    "        Args:\n",
    "            model_id (str): Identifier for the pre-trained medical language model\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Import logging for better error tracking and debugging\n",
    "            import logging\n",
    "\n",
    "            # Configure logging\n",
    "            logging.basicConfig(level=logging.INFO, \n",
    "                                format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "            self.logger = logging.getLogger(__name__)\n",
    "\n",
    "            # Log model initialization\n",
    "            self.logger.info(f\"Initializing Medical Chatbot with model: {model_id}\")\n",
    "\n",
    "            # Create a text generation pipeline using the specified model\n",
    "            self.pipeline = transformers.pipeline(\n",
    "                \"text-generation\",            # Specify the task as text generation\n",
    "                model=model_id,               # Use the specified model from Hugging Face\n",
    "                model_kwargs={\"torch_dtype\": torch.bfloat16},  # Use lower precision for memory efficiency\n",
    "                device=\"auto\",                # Automatically select best available device (CPU/GPU)\n",
    "            )\n",
    "        # Log successful model loading\n",
    "            self.logger.info(\"Model successfully initialized\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # More robust error handling\n",
    "            self.logger.error(f\"Failed to initialize model: {e}\")\n",
    "            raise RuntimeError(f\"Model initialization failed: {e}\")\n",
    "        \n",
    "\n",
    "        # Default medical category (can be dynamically updated later)\n",
    "        self.med_cat = \"Basic Medical Knowledge\"\n",
    "\n",
    "        # Fetch the initial system prompt based on the category\n",
    "        self.system_prompt = self.category_prompts.get(self.med_cat, self.default_prompt)\n",
    "\n",
    "    def set_category(self, category: str):\n",
    "        \"\"\"\n",
    "        Set the medical category dynamically and update the system prompt.\n",
    "        \n",
    "        Args:\n",
    "            category (str): New medical category to set.\n",
    "        \"\"\"\n",
    "        self.med_cat = category\n",
    "        self.system_prompt = self.category_prompts.get(self.med_cat, self.default_prompt)\n",
    "\n",
    "    def generate_response(\n",
    "        self, \n",
    "        user_query: str,           # Type hint for user's input query\n",
    "        max_tokens: int = 256,     # Default maximum response length\n",
    "        temperature: float = 0.1,   # Default temperature for response variability\n",
    "        safety_threshold: float = 0.7  # New parameter for response safety\n",
    "    ) -> str:                      # Type hint indicating return is a string\n",
    "        \"\"\"\n",
    "        Generate a medical response to a user's query\n",
    "        \n",
    "        Args:\n",
    "            user_query (str): Medical question or prompt\n",
    "            max_tokens (int): Maximum response length\n",
    "            temperature (float): Controls response randomness/creativity\n",
    "        \n",
    "        Returns:\n",
    "            str: Generated medical response\n",
    "        \"\"\"\n",
    "\n",
    "        # Input validation\n",
    "        if not user_query or not isinstance(user_query, str):\n",
    "            raise ValueError(\"User query must be a non-empty string\")\n",
    "        \n",
    "        # Ensure temperature is within a reasonable range\n",
    "        temperature = max(0.0, min(temperature, 1.0))\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Construct message list with system and user roles\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},  # System prompt defining AI's role\n",
    "                {\"role\": \"user\", \"content\": user_query}             # User query for the model\n",
    "            ]\n",
    "\n",
    "            # Apply chat template to format messages for the model\n",
    "            prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,               # Return as string, not tokens\n",
    "                add_generation_prompt=True    # Add markers for response generation\n",
    "            )\n",
    "\n",
    "            # Define token IDs to terminate generation\n",
    "            terminators = [\n",
    "                self.pipeline.tokenizer.eos_token_id,                 # Standard end-of-sequence token\n",
    "                self.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  # Custom end token\n",
    "            ]\n",
    "\n",
    "            # Generate response using the language model\n",
    "            outputs = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=max_tokens,    # Limit response length\n",
    "                eos_token_id=terminators,     # Use defined termination tokens\n",
    "                do_sample=True,               # Enable probabilistic sampling\n",
    "                temperature=temperature,       # Control response randomness\n",
    "                top_p=0.9,                    # Nucleus sampling parameter\n",
    "                top_k=50,  # Added top-k sampling for more controlled generation\n",
    "            )\n",
    "\n",
    "            # Extract and clean the generated response\n",
    "            generated_text = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "            # Optional: Simple safety filtering (very basic, consider more robust solutions)\n",
    "            if len(generated_text.split()) / max_tokens > safety_threshold:\n",
    "                self.logger.warning(\"Generated response might exceed safety threshold\")\n",
    "\n",
    "            return generated_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Comprehensive error logging\n",
    "            self.logger.error(f\"Response generation failed: {e}\")\n",
    "            return f\"An error occurred while processing your query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predefined medical questions for testing the model catogorized with dictionaries \n",
    "medical_questions = {\n",
    "    \"Basic Medical Knowledge\": [\n",
    "        \"What are the primary symptoms of type 2 diabetes?\",\n",
    "        \"Explain the pathophysiology of hypertension.\",\n",
    "        \"What are the recommended screening protocols for breast cancer?\"\n",
    "    ],\n",
    "    \"Pharmacological Queries\": [\n",
    "        \"What are the potential side effects of statins?\",\n",
    "        \"How do ACE inhibitors work to manage blood pressure?\",\n",
    "        \"Compare the mechanisms of different antidepressant classes.\"\n",
    "    ],\n",
    "    \"Diagnostic Reasoning\": [\n",
    "        \"What diagnostic tests would you recommend for suspected rheumatoid arthritis?\",\n",
    "        \"Describe the differential diagnosis for chest pain in a 45-year-old male.\"\n",
    "    ],\n",
    "    \"Treatment & Management\": [\n",
    "        \"What are current guidelines for managing type 1 diabetes in adolescents?\",\n",
    "        \"Explain the stages of cancer treatment and potential therapies.\"\n",
    "    ],\n",
    "    \"Specialized Medical Topics\": [\n",
    "        \"How does CRISPR technology potentially impact genetic disease treatment?\",\n",
    "        \"What are the latest advances in immunotherapy for cancer?\"\n",
    "    ],\n",
    "    \"Preventive Medicine\": [\n",
    "        \"What lifestyle modifications can reduce the risk of cardiovascular disease?\",\n",
    "        \"Discuss the importance of vaccination in preventing infectious diseases.\"\n",
    "    ],\n",
    "    \"Specialized Medical Scenarios\": [\n",
    "        \"What are the complications of untreated gestational diabetes?\",\n",
    "        \"Explain the neurological manifestations of multiple sclerosis.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "med_cat = \"Basic Medical Knowledge\" #select the medical category for the user query\n",
    "User_questions = medical_questions[med_cat] #set the users questions to the selected category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to demonstrate the chatbot's functionality\n",
    "def main():\n",
    "    try:\n",
    "         \n",
    "        # Create an instance of the MedicalChatbot\n",
    "        chatbot = MedicalChatbot()\n",
    "\n",
    "        # Support processing multiple categories\n",
    "        for category, questions in medical_questions.items():\n",
    "            print(f\"\\nü©∫ Category: {category}\")\n",
    "\n",
    "        # Iterate through a subset of medical questions\n",
    "            for question in User_questions:  # Test the selected questions\n",
    "                try:\n",
    "                        print(f\"\\nüìù Query: {question}\")\n",
    "                        response = chatbot.generate_response(question)\n",
    "                        print(f\"üìù Response: {response}\")\n",
    "                        print(\"-\" * 50)\n",
    "                except Exception as question_error:\n",
    "                    print(f\"Error processing question: {question_error}\")\n",
    "\n",
    "    except Exception as main_error:\n",
    "         print(f\"Critical error in main execution: {main_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the main function only runs if the script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 20:44:36,775 - INFO - Initializing Medical Chatbot with model: aaditya/OpenBioLLM-Llama3-70B\n",
      "2024-12-18 20:44:56,885 - INFO - Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-12-18 20:44:56,887 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478e76b90fa04a96bb3af41a6891e659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3971d8767bf436897187c492ddcd266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00030.bin:  34%|###4      | 1.56G/4.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyzwh\\OneDrive\\AI\\bootcamp\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tyzwh\\.cache\\huggingface\\hub\\models--aaditya--OpenBioLLM-Llama3-70B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb498f1dfc0b4bd2ab962d583632b43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3229aeb9c4564925b866e859a5145cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00030.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639c581391d742f4b7463759ad7d279e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00030.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f3a147b8684dbca3250ab605547de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9311302145324fb681ebab6ffebe777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30d90fb49e04fb2bb49c63569f32b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b034304a9cd4c2e9ad84cf29e007928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00008-of-00030.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ad2d48de584115b06da55481b0efc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00009-of-00030.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d7ff2a3b3946678d42a75f24b0ab36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00010-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23a6c9767214eb39c6ae6ab003de94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00011-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7f12a17b2245949bdf880be11db492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00012-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3ded0992a54be4aa9cccb956436159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00013-of-00030.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38af7c9638b4bdf9f892b9889e48586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00014-of-00030.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff61f99ba184108b1c37ab71761e302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00015-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5053657c00464880b116af9ae0c38594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00016-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b9e47dc66a4d0291d60272553166ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00017-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f8b8da09934b6bb9e56a6ebe1cfe3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00018-of-00030.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64b8ce19190419db6127ec1bbcf6761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00019-of-00030.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c36ed305a043d1977b7e3acb49b5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00020-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf53abd6d50241cdb212243e126fe46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00021-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b947e883bb214c0ab212e782d2a0dfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00022-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cd337fb1be4ad082a4ab138841503a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00023-of-00030.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00570e546abe4b7aac2bd91a84b064ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00024-of-00030.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbd0f94639348abad13d89a3e6a8738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00025-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ba6e8e8e404c80824d875e288a8676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00026-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fabc438e38f462088135684a5bc7e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00027-of-00030.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53327c0760404e4fba2bdee60cebd0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00028-of-00030.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35528e07c2ca429395327d4cdde293aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00029-of-00030.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22b69c6b7294bef8612cca832d0a493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00030-of-00030.bin:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/e6/ef/e6ef22fa4d8cf7165fad5efd5bb7c80f909046e18d6d90669e7a2a38a85dc485/1d3e5e7ac799a39688899311c6a2c926dc6be33e2ef73bb38ddeff2a049b9057?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model-00030-of-00030.bin%3B+filename%3D%22pytorch_model-00030-of-00030.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1734829787&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDgyOTc4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U2L2VmL2U2ZWYyMmZhNGQ4Y2Y3MTY1ZmFkNWVmZDViYjdjODBmOTA5MDQ2ZTE4ZDZkOTA2NjllN2EyYTM4YTg1ZGM0ODUvMWQzZTVlN2FjNzk5YTM5Njg4ODk5MzExYzZhMmM5MjZkYzZiZTMzZTJlZjczYmIzOGRkZWZmMmEwNDliOTA1Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=QAx4patNX9b2-KEYOJagGpouCwoIfPY8auqVpf9hsz4804-wXRg%7EIoh2bDGfFSuipsQtwDMY2rqSQ54ZOZU5gSB4So6X%7ECKf3VNQzhl0MHdHJ3wGoEqPdV95msMUKJtcRAETKohdNU3SxVJ2JRbmsRgEDEt2Z9LjamC0Rt7Pb-6LN7WunM8yX-7SVn72x1sbrhtlVcDzfNfjl0yMY9FDKPS5lcs%7EkE%7Ejlp33V3VJGcEUuWaRVqFpxTuPm2%7E-TQLfXGqeA7ZwOjc9pfa1T0VEl%7EJIgFZMvRBv6QvCOuV9V-j-Flk0sT4t0EHH3kPBp0uBp5v%7E6fvK4FuphoPxE3lziQ__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "2024-12-18 22:13:37,381 - WARNING - Error while downloading from https://cdn-lfs-us-1.hf.co/repos/e6/ef/e6ef22fa4d8cf7165fad5efd5bb7c80f909046e18d6d90669e7a2a38a85dc485/1d3e5e7ac799a39688899311c6a2c926dc6be33e2ef73bb38ddeff2a049b9057?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model-00030-of-00030.bin%3B+filename%3D%22pytorch_model-00030-of-00030.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1734829787&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDgyOTc4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U2L2VmL2U2ZWYyMmZhNGQ4Y2Y3MTY1ZmFkNWVmZDViYjdjODBmOTA5MDQ2ZTE4ZDZkOTA2NjllN2EyYTM4YTg1ZGM0ODUvMWQzZTVlN2FjNzk5YTM5Njg4ODk5MzExYzZhMmM5MjZkYzZiZTMzZTJlZjczYmIzOGRkZWZmMmEwNDliOTA1Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=QAx4patNX9b2-KEYOJagGpouCwoIfPY8auqVpf9hsz4804-wXRg%7EIoh2bDGfFSuipsQtwDMY2rqSQ54ZOZU5gSB4So6X%7ECKf3VNQzhl0MHdHJ3wGoEqPdV95msMUKJtcRAETKohdNU3SxVJ2JRbmsRgEDEt2Z9LjamC0Rt7Pb-6LN7WunM8yX-7SVn72x1sbrhtlVcDzfNfjl0yMY9FDKPS5lcs%7EkE%7Ejlp33V3VJGcEUuWaRVqFpxTuPm2%7E-TQLfXGqeA7ZwOjc9pfa1T0VEl%7EJIgFZMvRBv6QvCOuV9V-j-Flk0sT4t0EHH3kPBp0uBp5v%7E6fvK4FuphoPxE3lziQ__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475e97e22bcf4c81b80839068a94bc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00030-of-00030.bin:  51%|#####1    | 1.08G/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#optional main function that allows users to input the category\n",
    "def main():\n",
    "    try:\n",
    "        # Create an instance of the MedicalChatbot\n",
    "        chatbot = MedicalChatbot()\n",
    "\n",
    "        # Print available categories\n",
    "        print(\"Available Medical Categories:\")\n",
    "        for idx, category in enumerate(medical_questions.keys(), 1):\n",
    "            print(f\"{idx}. {category}\")\n",
    "\n",
    "        # Get user input for category selection\n",
    "        while True:\n",
    "            try:\n",
    "                selection = input(\"\\nEnter the number of the category you want to query (or 'q' to quit): \")\n",
    "                \n",
    "                # Allow quitting\n",
    "                if selection.lower() == 'q':\n",
    "                    break\n",
    "\n",
    "                # Convert selection to integer and get category\n",
    "                category_list = list(medical_questions.keys())\n",
    "                selected_category = category_list[int(selection) - 1]\n",
    "\n",
    "                print(f\"\\nü©∫ Selected Category: {selected_category}\")\n",
    "\n",
    "                # Get questions for the selected category\n",
    "                category_questions = medical_questions[selected_category]\n",
    "\n",
    "                # Process questions in the selected category\n",
    "                for question in category_questions:\n",
    "                    try:\n",
    "                        print(f\"\\nüìù Query: {question}\")\n",
    "                        \n",
    "                        # Dynamically set the chatbot's category\n",
    "                        chatbot.set_category(selected_category)\n",
    "                        \n",
    "                        response = chatbot.generate_response(question)\n",
    "                        print(f\"üìù Response: {response}\")\n",
    "                        print(\"-\" * 50)\n",
    "                    except Exception as question_error:\n",
    "                        print(f\"Error processing question: {question_error}\")\n",
    "\n",
    "            except (ValueError, IndexError):\n",
    "                print(\"Invalid selection. Please enter a valid category number.\")\n",
    "\n",
    "    except Exception as main_error:\n",
    "        print(f\"Critical error in main execution: {main_error}\")\n",
    "\n",
    "# Keep the __main__ check\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

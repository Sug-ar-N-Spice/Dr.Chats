{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sug-ar-N-Spice/Dr.Chats/blob/Ty/New_Q%26A_Med_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Maa4226XzrVp",
        "outputId": "c0a712ec-01bd-44bf-a97c-ababcfbd1832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.45.2)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.4.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.25.2)\n",
            "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXoCugqJxbF0",
        "outputId": "794532b1-4b17-44b8-be16-56ba564a1a68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sacremoses in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (0.1.1)\n",
            "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses) (2023.10.3)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses) (4.66.4)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (4.45.2)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.25.2)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (2.4.1+cu124)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        " ! pip install sacremoses\n",
        " ! pip install transformers\n",
        " ! pip install datasets\n",
        " ! pip install torch\n",
        "#!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uQF6FdCExbF1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\jerom\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
        "from typing import List, Dict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CISRX77YYsgr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oVqt8RX33K36"
      },
      "outputs": [],
      "source": [
        "\n",
        "##STOP WORDS IN NLP DONT MEAN ANYTHING LIKE WE THEY THEY JUST COMPLETE THE SENTENCE\n",
        "\n",
        "## THIS IS CLASS THAT Cleans the data\n",
        "class MEDDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessor for general medical data.\n",
        "    This class handles cleaning, normalization, and preparation of text data\n",
        "    related to medical topics for use in a medical chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the preprocessor with necessary NLTK downloads.\"\"\"\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Minimal list of medical terms to preserve\n",
        "        self.medical_terms = {\n",
        "            # Terms conflicting with stopwords\n",
        "            'a', 'am', 'an', 'as', 'at', 'be', 'by', 'in', 'no', 'on', 'or', 'to', 'up',\n",
        "            # Critical abbreviations to always preserve\n",
        "            'ct', 'dr', 'er', 'hiv', 'hr', 'icu', 'iv', 'mr', 'ms'\n",
        "        }\n",
        "\n",
        "        self.stop_words = self.stop_words - self.medical_terms\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Clean and normalize the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned and normalized text\n",
        "        \"\"\"\n",
        "\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters but keep medical symbols\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s+\\-/%]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_stopwords(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove stopwords from the text, keeping medical specific terms.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            str: Text with stopwords removed\n",
        "        \"\"\"\n",
        "        words = word_tokenize(text) ### this is resulting in a list of words was converting sentence / paragraph into a list of words\n",
        "\n",
        "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Preprocess the entire dataframe.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input dataframe\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Preprocessed dataframe\n",
        "        \"\"\"\n",
        "        # change columns names depending which csv file you are using cleaning text and removing stopwords\n",
        "        df['clean_question'] = df['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        df['clean_context'] = df['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "\n",
        "        # Combine cleaned abstract and results or full_texts depending on which csv you are using\n",
        "        df['combined_text'] = df['clean_question'] + ' ' + df['clean_context']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_for_model(self, text: str, max_length: int = 512) -> str: ##looks at paragraph, cuts the paragraph if more than 512 This takes the sentence splits to words and has a max length\n",
        "        \"\"\"\n",
        "        Prepare text for model input, truncating if necessary.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_length (int): Maximum number of words\n",
        "\n",
        "        Returns:\n",
        "            str: Prepared text\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) > max_length:\n",
        "            return ' '.join(words[:max_length])\n",
        "        return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kwDrllroAuNF"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataframe(self, df): ###USUALLY YOU SEE SELF IN A CLASS This allows you to code attributes in a class\n",
        "    \"\"\"\n",
        "    Preprocess the entire dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame or DatasetDict): Input dataframe with 'question' and 'context' columns\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Preprocessed dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if df is a DatasetDict object-- When we try to find dataset Dict- that means we havent converted it to pandas and cleaning whole dataset\n",
        "\n",
        "\n",
        "    if isinstance(df.DatasetDict):\n",
        "        # Process each split separately and combine into a DataFrame\n",
        "        all_data = []\n",
        "        for split in df:\n",
        "                    # Assuming all splits have the same columns\n",
        "                    # Process the columns in each split\n",
        "            df_split = df[split].to_pandas()  # Convert to DataFrame\n",
        "\n",
        "            df_split['clean_question'] = df_split['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "            df_split['clean_context'] = df_split['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "            df_split['combined_text'] = df_split['clean_question'] + ' ' + df_split['clean_context']\n",
        "\n",
        "            all_data.extend(df_split.to_dict('records')) # Add processed data to the list\n",
        "\n",
        "        processed_df= pd.DataFrame(all_data) # Create a new DataFrame from the combined data\n",
        "\n",
        "    else:  # If it's a regular DataFrame, process as before\n",
        "        processed_df= df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
        "        processed_df['clean_question'] = processed_df['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        processed_df['clean_context'] = processed_df['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        processed_df['combined_text'] = processed_df['clean_question'] + ' ' + processed_df['clean_context']\n",
        "\n",
        "    return processed_df  # Return the processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i-9cw1Vx6-ku"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowSelector:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', window_size=1000, stride=500, batch_size=32):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.contexts = []\n",
        "        self.embeddings = []\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit(self, contexts):\n",
        "        for i in range(0, len(contexts), self.stride):\n",
        "            window = contexts[i:i+self.window_size]\n",
        "            self.contexts.extend(window)\n",
        "\n",
        "            # Process embeddings in batches\n",
        "            for j in range(0, len(window), self.batch_size):\n",
        "                batch = window[j:j+self.batch_size]\n",
        "                batch_embeddings = self.model.encode(batch)\n",
        "                self.embeddings.extend(batch_embeddings)\n",
        "\n",
        "        self.embeddings = np.array(self.embeddings)\n",
        "\n",
        "    def get_most_relevant_context(self, question, top_n=3):\n",
        "        question_embedding = self.model.encode([question])\n",
        "\n",
        "        # Calculate similarities in batches\n",
        "        all_similarities = []\n",
        "        for i in range(0, len(self.embeddings), self.batch_size):\n",
        "            batch_embeddings = self.embeddings[i:i+self.batch_size]\n",
        "            batch_similarities = np.dot(batch_embeddings, question_embedding.T).squeeze()\n",
        "            all_similarities.extend(batch_similarities)\n",
        "\n",
        "        top_indices = np.argsort(all_similarities)[-top_n:][::-1]\n",
        "        return [self.contexts[i] for i in top_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6L9HNuIxbF3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# class ConsistentVocabularyContextSelector:\n",
        "#     def __init__(self, chunk_size=1000):\n",
        "#         self.vectorizer = None\n",
        "#         self.tfidf_matrices = []\n",
        "#         self.contexts = []\n",
        "#         self.chunk_size = chunk_size\n",
        "\n",
        "#     def fit(self, contexts):\n",
        "#         # First, fit the vectorizer on all contexts to get a consistent vocabulary\n",
        "#         self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "#         self.vectorizer.fit(contexts)\n",
        "\n",
        "#         # Now, transform chunks using the consistent vocabulary\n",
        "#         for i in range(0, len(contexts), self.chunk_size):\n",
        "#             chunk = contexts[i:i+self.chunk_size]\n",
        "#             self.contexts.extend(chunk)\n",
        "#             tfidf_matrix = self.vectorizer.transform(chunk)\n",
        "#             self.tfidf_matrices.append(tfidf_matrix)\n",
        "\n",
        "#     def get_most_relevant_context(self, question, top_n=1):\n",
        "#         question_vector = self.vectorizer.transform([question])\n",
        "#         all_similarities = []\n",
        "#         for tfidf_matrix in self.tfidf_matrices:\n",
        "#             similarities = cosine_similarity(question_vector, tfidf_matrix)\n",
        "#             all_similarities.extend(similarities[0])\n",
        "\n",
        "#         most_similar_idx = np.argsort(all_similarities)[-top_n:][::-1]\n",
        "#         return [self.contexts[i] for i in most_similar_idx]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TKamxcy0BL50"
      },
      "outputs": [],
      "source": [
        "class DRChatbot:\n",
        "    def __init__(self, qa_model=\"microsoft/BioGPT\", window_size=1000, stride=500, batch_size=32):\n",
        "        self.preprocessor = MEDDataPreprocessor()\n",
        "        self.qa_tokenizer = AutoTokenizer.from_pretrained(qa_model)\n",
        "        self.qa_model = AutoModelForCausalLM.from_pretrained(qa_model)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.qa_model.to(self.device)\n",
        "        self.context_selector = SlidingWindowSelector(window_size=window_size, stride=stride, batch_size=batch_size)\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        processed_df = self.preprocessor.preprocess_dataframe(df)\n",
        "        return processed_df['combined_text'].tolist()\n",
        "\n",
        "    def fit_context_selector(self, all_contexts):\n",
        "        self.context_selector.fit(all_contexts)\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        relevant_contexts = self.context_selector.get_most_relevant_context(question, top_n=3)\n",
        "        combined_context = \" \".join(relevant_contexts)\n",
        "\n",
        "        prompt = f\"Based on the following medical information:\\n{combined_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "        inputs = self.qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        max_new_tokens = min(100, 1024 - input_length)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.qa_model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "        return self.qa_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sWZG8b3dBgOr"
      },
      "outputs": [],
      "source": [
        "def process_large_csv(file_path, window_size=1000, stride=500, batch_size=32):\n",
        "    chatbot = DRChatbot(window_size=window_size, stride=stride, batch_size=batch_size)\n",
        "    all_contexts = []\n",
        "\n",
        "    # Process the CSV file in chunks to handle large files\n",
        "    for chunk in pd.read_csv(file_path, chunksize=window_size, sep='\\t'):\n",
        "        contexts = chatbot.preprocess_data(chunk)\n",
        "        all_contexts.extend(contexts)\n",
        "\n",
        "    chatbot.fit_context_selector(all_contexts)\n",
        "    return chatbot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51aDH1g2F86a",
        "outputId": "0bb7214b-9cb3-434f-e696-0bf69037700e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b01YYSneB-2I",
        "outputId": "50ade08f-de47-49fd-c2d0-464753deb6b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jerom\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "file_path = './ai_medical_dataset.csv'\n",
        "chatbot = process_large_csv(file_path, window_size=1000, stride=500, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chatbot.save_model('ai_medical_chatbot.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXeeMzXYBi9a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Ask a question\n",
        "question = \"What are the symptoms of asthma?\"\n",
        "answer = chatbot.answer_question(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U91BB7uxbF3"
      },
      "outputs": [],
      "source": [
        "# class DRChatbot:\n",
        "#     def __init__(self, qa_model=\"microsoft/BioGPT\"):\n",
        "#         self.preprocessor = MEDDataPreprocessor()\n",
        "#         self.qa_tokenizer = AutoTokenizer.from_pretrained(qa_model)\n",
        "#         self.qa_model = AutoModelForCausalLM.from_pretrained(qa_model)\n",
        "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#         self.qa_model.to(self.device)\n",
        "#         self.context_selector = ConsistentVocabularyContextSelector()\n",
        "\n",
        "#     def preprocess_data(self, df):\n",
        "#         processed_df = self.preprocessor.preprocess_dataframe(df)\n",
        "#         return processed_df['combined_text'].tolist()\n",
        "\n",
        "#     def fit_context_selector(self, all_contexts):\n",
        "#         self.context_selector.fit(all_contexts)\n",
        "\n",
        "#     def answer_question(self, question):\n",
        "#         relevant_contexts = self.context_selector.get_most_relevant_context(question, top_n=3)\n",
        "#         combined_context = \" \".join(relevant_contexts)\n",
        "\n",
        "#         prompt = f\"Based on the following medical information:\\n{combined_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "#         inputs = self.qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "#         input_length = inputs[\"input_ids\"].shape[1]\n",
        "#         max_new_tokens = min(100, 1024 - input_length)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             output = self.qa_model.generate(\n",
        "#                 inputs[\"input_ids\"],\n",
        "#                 max_new_tokens=max_new_tokens,\n",
        "#                 num_return_sequences=1,\n",
        "#                 do_sample=True,\n",
        "#                 temperature=0.7\n",
        "#             )\n",
        "\n",
        "#         return self.qa_tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAiZFXaIxbF3"
      },
      "outputs": [],
      "source": [
        "# Process the large CSV file\n",
        "chatbot = process_large_csv('processed_df_1.csv', window_size=1000, stride=500, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF5sl_baxbF4",
        "outputId": "0e8f3ead-8c8b-4fff-fc5b-24b4fe3bdd4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: what are the symptons of asthma?\n",
            "A: Based on the following medical information: name group a member asthma asthma impact asthma exacerbations asthma impact asthma exacerbations asthma triggers on asthma-related quality life in patients severe or difficult-to-treat asthma goal national asthma guidelines background maintenance asthma control time a clear goal national asthma guidelines studies addressed natural history asthma control time objective to assess impairment domain asthma control time in patients persistent asthma to determine predictors consequences controlled uncontrolled asthma time methods patients 18-56 years old persistent asthma completed baseline november 2007 follow-up asthma surveys april july october 2008 included in study survey included asthma control test as well as questions regarding patient asthma characteristics health care utilization pharmacy exacerbations 2008 obtained administrative data results baseline first follow-up surveys completed by 1267 patients 4 surveys completed by 782 patients patients well-controlled asthma at baseline significantly likely p 0001 to well-controlled asthma following year 762% -804% patients uncontrolled asthma at baseline 335% -369% patients whose asthma control improved first several months follow-up experienced significantly p 05 fewer exacerbations subsequent year patients initially uncontrolled asthma improve conclusion degree asthma control at one point in time strongly related to achievement or maintenance control to asthma exacerbations time patients uncontrolled asthma especially poorly controlled asthma receive intensive management follow-up in an attempt to achieve well-controlled asthma time Question: what are the symptons of asthma? Answer: The following asthma control questions are: \"What is the symptons of asthma?\" Answer: \"There is a lack of asthma control in the adult population that is not related to the fact of asthma but is the result of the asthma in which the person has not been diagnosed.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "question = \"what are the symptons of asthma?\"\n",
        "answer = chatbot.answer_question(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sug-ar-N-Spice/Dr.Chats/blob/Patricia/Patricia_Dr_chat_pre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXoCugqJxbF0",
        "outputId": "6022b1c8-ddd8-4496-ac17-1b2d05bb9fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.2.1\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.5)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "! pip install sacremoses\n",
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install torch\n",
        "#!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uQF6FdCExbF1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
        "from typing import List, Dict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CISRX77YYsgr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oVqt8RX33K36"
      },
      "outputs": [],
      "source": [
        "\n",
        "##STOP WORDS IN NLP DONT MEAN ANYTHING LIKE WE THEY THEY JUST COMPLETE THE SENTENCE\n",
        "\n",
        "## THIS IS CLASS THAT Cleans the data\n",
        "class MEDDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessor for general medical data.\n",
        "    This class handles cleaning, normalization, and preparation of text data\n",
        "    related to medical topics for use in a medical chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the preprocessor with necessary NLTK downloads.\"\"\"\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Minimal list of medical terms to preserve\n",
        "        self.medical_terms = {\n",
        "            # Terms conflicting with stopwords\n",
        "            'a', 'am', 'an', 'as', 'at', 'be', 'by', 'in', 'no', 'on', 'or', 'to', 'up',\n",
        "            # Critical abbreviations to always preserve\n",
        "            'ct', 'dr', 'er', 'hiv', 'hr', 'icu', 'iv', 'mr', 'ms'\n",
        "        }\n",
        "\n",
        "        self.stop_words = self.stop_words - self.medical_terms\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Clean and normalize the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned and normalized text\n",
        "        \"\"\"\n",
        "\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters but keep medical symbols\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s+\\-/%]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_stopwords(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove stopwords from the text, keeping medical specific terms.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            str: Text with stopwords removed\n",
        "        \"\"\"\n",
        "        words = word_tokenize(text) ### this is resulting in a list of words was converting sentence / paragraph into a list of words\n",
        "\n",
        "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Preprocess the entire dataframe.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input dataframe\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Preprocessed dataframe\n",
        "        \"\"\"\n",
        "        # change columns names depending which csv file you are using cleaning text and removing stopwords\n",
        "        df['clean_question'] = df['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        df['clean_context'] = df['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "\n",
        "        # Combine cleaned abstract and results or full_texts depending on which csv you are using\n",
        "        df['combined_text'] = df['clean_question'] + ' ' + df['clean_context']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_for_model(self, text: str, max_length: int = 512) -> str: ##looks at paragraph, cuts the paragraph if more than 512 This takes the sentence splits to words and has a max length\n",
        "        \"\"\"\n",
        "        Prepare text for model input, truncating if necessary.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_length (int): Maximum number of words\n",
        "\n",
        "        Returns:\n",
        "            str: Prepared text\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) > max_length:\n",
        "            return ' '.join(words[:max_length])\n",
        "        return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kwDrllroAuNF"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataframe(self, df): ###USUALLY YOU SEE SELF IN A CLASS This allows you to code attributes in a class\n",
        "    \"\"\"\n",
        "    Preprocess the entire dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame or DatasetDict): Input dataframe with 'question' and 'context' columns\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Preprocessed dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if df is a DatasetDict object-- When we try to find dataset Dict- that means we havent converted it to pandas and cleaning whole dataset\n",
        "\n",
        "\n",
        "    if isinstance(df.DatasetDict):\n",
        "        # Process each split separately and combine into a DataFrame\n",
        "        all_data = []\n",
        "        for split in df:\n",
        "                    # Assuming all splits have the same columns\n",
        "                    # Process the columns in each split\n",
        "            df_split = df[split].to_pandas()  # Convert to DataFrame\n",
        "\n",
        "            df_split['clean_question'] = df_split['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "            df_split['clean_context'] = df_split['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "            df_split['combined_text'] = df_split['clean_question'] + ' ' + df_split['clean_context']\n",
        "\n",
        "            all_data.extend(df_split.to_dict('records')) # Add processed data to the list\n",
        "\n",
        "        processed_df= pd.DataFrame(all_data) # Create a new DataFrame from the combined data\n",
        "\n",
        "    else:  # If it's a regular DataFrame, process as before\n",
        "        processed_df= df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
        "        processed_df['clean_question'] = processed_df['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        processed_df['clean_context'] = processed_df['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        processed_df['combined_text'] = processed_df['clean_question'] + ' ' + processed_df['clean_context']\n",
        "\n",
        "    return processed_df  # Return the processed data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SlidingWindowSelector:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', window_size=500, stride=250, batch_size=16):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.contexts = []\n",
        "        self.embeddings = []\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit(self, contexts):\n",
        "        for i in range(0, len(contexts), self.stride):\n",
        "            window = contexts[i:i+self.window_size]\n",
        "            self.contexts.extend(window)\n",
        "\n",
        "            # Process embeddings in batches\n",
        "            for j in range(0, len(window), self.batch_size):\n",
        "                batch = window[j:j+self.batch_size]\n",
        "                batch_embeddings = self.model.encode(batch)\n",
        "                self.embeddings.extend(batch_embeddings)\n",
        "\n",
        "        self.embeddings = np.array(self.embeddings)\n",
        "\n",
        "    def update(self, new_contexts):\n",
        "        new_embeddings = []\n",
        "        for i in range(0, len(new_contexts), self.batch_size):\n",
        "            batch = new_contexts[i:i+self.batch_size]\n",
        "            batch_embeddings = self.model.encode(batch)\n",
        "            new_embeddings.extend(batch_embeddings)\n",
        "\n",
        "        self.contexts.extend(new_contexts)\n",
        "        self.embeddings = np.vstack([self.embeddings, np.array(new_embeddings)])\n",
        "\n",
        "    def get_most_relevant_context(self, question, top_n=3):\n",
        "        question_embedding = self.model.encode([question])\n",
        "\n",
        "        # Calculate similarities in batches\n",
        "        all_similarities = []\n",
        "        for i in range(0, len(self.embeddings), self.batch_size):\n",
        "            batch_embeddings = self.embeddings[i:i+self.batch_size]\n",
        "            batch_similarities = np.dot(batch_embeddings, question_embedding.T).squeeze()\n",
        "            all_similarities.extend(batch_similarities)\n",
        "\n",
        "        top_indices = np.argsort(all_similarities)[-top_n:][::-1]\n",
        "        return [self.contexts[i] for i in top_indices]\n"
      ],
      "metadata": {
        "id": "i-9cw1Vx6-ku"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DRChatbot:\n",
        "    def __init__(self, qa_model=\"microsoft/BioGPT\", window_size=500, stride=250, batch_size=16):\n",
        "        self.preprocessor = MEDDataPreprocessor()\n",
        "        self.qa_tokenizer = AutoTokenizer.from_pretrained(qa_model)\n",
        "        self.qa_model = AutoModelForCausalLM.from_pretrained(qa_model)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.qa_model.to(self.device)\n",
        "        self.context_selector = SlidingWindowSelector(window_size=window_size, stride=stride, batch_size=batch_size)\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        processed_df = self.preprocessor.preprocess_dataframe(df)\n",
        "        return processed_df['combined_text'].tolist()\n",
        "\n",
        "    def fit_context_selector(self, all_contexts):\n",
        "        self.context_selector.fit(all_contexts)\n",
        "\n",
        "    def update_context_selector(self, new_contexts):\n",
        "        self.context_selector.update(new_contexts)\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        relevant_contexts = self.context_selector.get_most_relevant_context(question, top_n=3)\n",
        "        combined_context = \" \".join(relevant_contexts)\n",
        "\n",
        "        prompt = f\"Based on the following medical information:\\n{combined_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "        inputs = self.qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        max_new_tokens = min(100, 1024 - input_length)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.qa_model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "        return self.qa_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "TKamxcy0BL50"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_large_csv(file_path, window_size=500, stride=250, batch_size=16, chunk_size=10000):\n",
        "    chatbot = DRChatbot(window_size=window_size, stride=stride, batch_size=batch_size)\n",
        "    all_contexts = []\n",
        "\n",
        "     # Process the CSV file in chunks\n",
        "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
        "        contexts = chatbot.preprocess_data(chunk)\n",
        "        if i == 0:\n",
        "            chatbot.fit_context_selector(contexts)\n",
        "        else:\n",
        "            chatbot.update_context_selector(contexts)\n",
        "        print(f\"Processed chunk {i+1}, total contexts: {len(chatbot.context_selector.contexts)}\")\n",
        "\n",
        "    return chatbot\n",
        "\n"
      ],
      "metadata": {
        "id": "sWZG8b3dBgOr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51aDH1g2F86a",
        "outputId": "f55598da-56df-4ae8-bfb9-a2f679524979"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "file_path = '/content/drive/My Drive/million_sample.csv'\n",
        "chatbot = process_large_csv(file_path, window_size=500, stride=250, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b01YYSneB-2I",
        "outputId": "feac575d-5b38-4b10-8c2e-761860bb3d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 1, total contexts: 19750\n",
            "Processed chunk 2, total contexts: 29750\n",
            "Processed chunk 3, total contexts: 39750\n",
            "Processed chunk 4, total contexts: 49750\n",
            "Processed chunk 5, total contexts: 59750\n",
            "Processed chunk 6, total contexts: 69750\n",
            "Processed chunk 7, total contexts: 79750\n",
            "Processed chunk 8, total contexts: 89750\n",
            "Processed chunk 9, total contexts: 99750\n",
            "Processed chunk 10, total contexts: 109750\n",
            "Processed chunk 11, total contexts: 119750\n",
            "Processed chunk 12, total contexts: 129750\n",
            "Processed chunk 13, total contexts: 139750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Ask a question\n",
        "question = \"What are the symptoms of asthma?\"\n",
        "answer = chatbot.answer_question(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")"
      ],
      "metadata": {
        "id": "MXeeMzXYBi9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAiZFXaIxbF3"
      },
      "outputs": [],
      "source": [
        "# Process the large CSV file\n",
        "chatbot = process_large_csv('million_sample.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF5sl_baxbF4",
        "outputId": "0e8f3ead-8c8b-4fff-fc5b-24b4fe3bdd4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: what are the symptons of asthma?\n",
            "A: Based on the following medical information: name group a member asthma asthma impact asthma exacerbations asthma impact asthma exacerbations asthma triggers on asthma-related quality life in patients severe or difficult-to-treat asthma goal national asthma guidelines background maintenance asthma control time a clear goal national asthma guidelines studies addressed natural history asthma control time objective to assess impairment domain asthma control time in patients persistent asthma to determine predictors consequences controlled uncontrolled asthma time methods patients 18-56 years old persistent asthma completed baseline november 2007 follow-up asthma surveys april july october 2008 included in study survey included asthma control test as well as questions regarding patient asthma characteristics health care utilization pharmacy exacerbations 2008 obtained administrative data results baseline first follow-up surveys completed by 1267 patients 4 surveys completed by 782 patients patients well-controlled asthma at baseline significantly likely p 0001 to well-controlled asthma following year 762% -804% patients uncontrolled asthma at baseline 335% -369% patients whose asthma control improved first several months follow-up experienced significantly p 05 fewer exacerbations subsequent year patients initially uncontrolled asthma improve conclusion degree asthma control at one point in time strongly related to achievement or maintenance control to asthma exacerbations time patients uncontrolled asthma especially poorly controlled asthma receive intensive management follow-up in an attempt to achieve well-controlled asthma time Question: what are the symptons of asthma? Answer: The following asthma control questions are: \"What is the symptons of asthma?\" Answer: \"There is a lack of asthma control in the adult population that is not related to the fact of asthma but is the result of the asthma in which the person has not been diagnosed.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "question = \"what are the symptons of asthma?\"\n",
        "answer = chatbot.answer_question(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
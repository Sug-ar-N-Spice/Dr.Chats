{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sug-ar-N-Spice/Dr.Chats/blob/Patricia/Patricia_Dr_chat_pre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXoCugqJxbF0",
        "outputId": "6022b1c8-ddd8-4496-ac17-1b2d05bb9fef"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "! pip install sacremoses\n",
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install torch\n",
        "#!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uQF6FdCExbF1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\tyzwh\\OneDrive\\AI\\bootcamp\\envs\\dev\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
        "from typing import List, Dict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import gc  #garbage collector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CISRX77YYsgr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oVqt8RX33K36"
      },
      "outputs": [],
      "source": [
        "\n",
        "##STOP WORDS IN NLP DONT MEAN ANYTHING LIKE WE THEY THEY JUST COMPLETE THE SENTENCE\n",
        "\n",
        "## THIS IS CLASS THAT Cleans the data\n",
        "class MEDDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessor for general medical data.\n",
        "    This class handles cleaning, normalization, and preparation of text data\n",
        "    related to medical topics for use in a medical chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the preprocessor with necessary NLTK downloads.\"\"\"\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Minimal list of medical terms to preserve\n",
        "        self.medical_terms = {\n",
        "            # Terms conflicting with stopwords\n",
        "            'a', 'am', 'an', 'as', 'at', 'be', 'by', 'in', 'no', 'on', 'or', 'to', 'up',\n",
        "            # Critical abbreviations to always preserve\n",
        "            'ct', 'dr', 'er', 'hiv', 'hr', 'icu', 'iv', 'mr', 'ms'\n",
        "        }\n",
        "\n",
        "        self.stop_words = self.stop_words - self.medical_terms\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Clean and normalize the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned and normalized text\n",
        "        \"\"\"\n",
        "\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters but keep medical symbols\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s+\\-/%]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_stopwords(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove stopwords from the text, keeping medical specific terms.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            str: Text with stopwords removed\n",
        "        \"\"\"\n",
        "        words = word_tokenize(text) ### this is resulting in a list of words was converting sentence / paragraph into a list of words\n",
        "\n",
        "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Preprocess the entire dataframe.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input dataframe\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Preprocessed dataframe\n",
        "        \"\"\"\n",
        "        # change columns names depending which csv file you are using cleaning text and removing stopwords\n",
        "        df['clean_question'] = df['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        df['clean_context'] = df['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "\n",
        "        # Combine cleaned abstract and results or full_texts depending on which csv you are using\n",
        "        df['combined_text'] = df['clean_question'] + ' ' + df['clean_context']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_for_model(self, text: str, max_length: int = 512) -> str: ##looks at paragraph, cuts the paragraph if more than 512 This takes the sentence splits to words and has a max length\n",
        "        \"\"\"\n",
        "        Prepare text for model input, truncating if necessary.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_length (int): Maximum number of words\n",
        "\n",
        "        Returns:\n",
        "            str: Prepared text\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) > max_length:\n",
        "            return ' '.join(words[:max_length])\n",
        "        return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kwDrllroAuNF"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataframe(self, df): ###USUALLY YOU SEE SELF IN A CLASS This allows you to code attributes in a class\n",
        "    \"\"\"\n",
        "    Preprocess the entire dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame or DatasetDict): Input dataframe with 'question' and 'context' columns\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Preprocessed dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if df is a DatasetDict object-- When we try to find dataset Dict- that means we havent converted it to pandas and cleaning whole dataset\n",
        "\n",
        "\n",
        "    if isinstance(df.DatasetDict):\n",
        "        # Process each split separately and combine into a DataFrame\n",
        "        all_data = []\n",
        "        for split in df:\n",
        "                    # Assuming all splits have the same columns\n",
        "                    # Process the columns in each split\n",
        "            df_split = df[split].to_pandas()  # Convert to DataFrame\n",
        "\n",
        "            df_split['clean_question'] = df_split['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "            df_split['clean_context'] = df_split['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "            df_split['combined_text'] = df_split['clean_question'] + ' ' + df_split['clean_context']\n",
        "\n",
        "            all_data.extend(df_split.to_dict('records')) # Add processed data to the list\n",
        "\n",
        "        processed_df= pd.DataFrame(all_data) # Create a new DataFrame from the combined data\n",
        "\n",
        "    else:  # If it's a regular DataFrame, process as before\n",
        "        processed_df= df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
        "        processed_df['clean_question'] = processed_df['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        processed_df['clean_context'] = processed_df['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        processed_df['combined_text'] = processed_df['clean_question'] + ' ' + processed_df['clean_context']\n",
        "\n",
        "    return processed_df  # Return the processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i-9cw1Vx6-ku"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowSelector:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', window_size=500, stride=250, batch_size=16):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.contexts = []\n",
        "        self.embeddings = []\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit(self, contexts):\n",
        "        if not contexts:\n",
        "            print(\"Warning: Empty contexts provided to fit\")\n",
        "            return\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"Fitting {len(contexts)} contexts\")\n",
        "        for i in range(0, len(contexts), self.stride):\n",
        "            window = contexts[i:i+self.window_size]\n",
        "            self.contexts.extend(window)\n",
        "\n",
        "            # Process embeddings in batches\n",
        "            for j in range(0, len(window), self.batch_size):\n",
        "                batch = window[j:j+self.batch_size]\n",
        "                batch_embeddings = self.model.encode(batch)\n",
        "                self.embeddings.extend(batch_embeddings)\n",
        "\n",
        "        self.embeddings = np.array(self.embeddings)\n",
        "        print(f\"After fit: {len(self.contexts)} contexts, embeddings shape: {self.embeddings.shape}\")\n",
        "\n",
        "    def update(self, new_contexts):\n",
        "        if not new_contexts:  # Check if new_contexts is empty\n",
        "            print(\"Warning: Received empty contexts, skipping update\")\n",
        "            return\n",
        "\n",
        "        print(f\"Updating with {len(new_contexts)} new contexts\")\n",
        "        print(f\"Before update: {len(self.contexts)} contexts, embeddings shape: {self.embeddings.shape if isinstance(self.embeddings, np.ndarray) else 'empty'}\")\n",
        "\n",
        "        new_embeddings = []\n",
        "        for i in range(0, len(new_contexts), self.batch_size):\n",
        "            batch = new_contexts[i:i+self.batch_size]\n",
        "            batch_embeddings = self.model.encode(batch)\n",
        "            new_embeddings.extend(batch_embeddings)\n",
        "\n",
        "        # Convert new_embeddings to numpy array\n",
        "        new_embeddings = np.array(new_embeddings)\n",
        "        print(f\"New embeddings shape: {new_embeddings.shape}\")\n",
        "\n",
        "        # Check if we have existing embeddings\n",
        "        if len(self.embeddings) == 0 or not isinstance(self.embeddings, np.ndarray):\n",
        "            self.embeddings = new_embeddings\n",
        "        else:\n",
        "            # Ensure dimensions match before stacking\n",
        "            if new_embeddings.size > 0:  # Only stack if we have new embeddings\n",
        "                try:\n",
        "                    self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error in update: Embedding shapes - existing: {self.embeddings.shape}, new: {new_embeddings.shape}\")\n",
        "                    raise\n",
        "\n",
        "        self.contexts.extend(new_contexts)\n",
        "        print(f\"After update: {len(self.contexts)} contexts, embeddings shape: {self.embeddings.shape}\")\n",
        "\n",
        "    def get_most_relevant_context(self, question, top_n=3):\n",
        "        question_embedding = self.model.encode([question])\n",
        "\n",
        "        # Calculate similarities in batches\n",
        "        all_similarities = []\n",
        "        for i in range(0, len(self.embeddings), self.batch_size):\n",
        "            batch_embeddings = self.embeddings[i:i+self.batch_size]\n",
        "            batch_similarities = np.dot(batch_embeddings, question_embedding.T).squeeze()\n",
        "            all_similarities.extend(batch_similarities)\n",
        "\n",
        "        top_indices = np.argsort(all_similarities)[-top_n:][::-1]\n",
        "        return [self.contexts[i] for i in top_indices]\n",
        "     # New methods for pickling functionality\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TKamxcy0BL50"
      },
      "outputs": [],
      "source": [
        "class DRChatbot:\n",
        "    def __init__(self, qa_model=\"microsoft/BioGPT\", window_size=500, stride=250, batch_size=16):\n",
        "        self.model_name = qa_model  # Store model name for loading from pickle\n",
        "        self.preprocessor = MEDDataPreprocessor()\n",
        "        self.qa_tokenizer = AutoTokenizer.from_pretrained(qa_model)\n",
        "        self.qa_model = AutoModelForCausalLM.from_pretrained(qa_model)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.qa_model.to(self.device)\n",
        "        self.context_selector = SlidingWindowSelector(window_size=window_size, stride=stride, batch_size=batch_size)\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        processed_df = self.preprocessor.preprocess_dataframe(df)\n",
        "        return processed_df['combined_text'].tolist()\n",
        "\n",
        "    def fit_context_selector(self, all_contexts):\n",
        "        self.context_selector.fit(all_contexts)\n",
        "\n",
        "    def update_context_selector(self, new_contexts):\n",
        "        self.context_selector.update(new_contexts)\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        relevant_contexts = self.context_selector.get_most_relevant_context(question, top_n=3)\n",
        "        combined_context = \" \".join(relevant_contexts)\n",
        "\n",
        "        prompt = f\"Based on the following medical information:\\n{combined_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "        inputs = self.qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        max_new_tokens = min(100, 1024 - input_length)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.qa_model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "        return self.qa_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    def save_state(self, save_dir: str, prefix: str = \"chunk\"):\n",
        "\n",
        "        \"\"\"Centralized save method\"\"\"\n",
        "\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Save complete state in a single file\n",
        "        state = {\n",
        "        'model_name': self.model_name,\n",
        "        'device': str(self.device),\n",
        "        'timestamp': timestamp,\n",
        "        'context_selector': {\n",
        "            'contexts': self.context_selector.contexts,\n",
        "            'embeddings': self.context_selector.embeddings,\n",
        "            'window_size': self.context_selector.window_size,\n",
        "            'stride': self.context_selector.stride,\n",
        "            'batch_size': self.context_selector.batch_size\n",
        "            }\n",
        "        }\n",
        "    \n",
        "        # Save as a single file\n",
        "        state_path = f\"{save_dir}/{prefix}_{timestamp}.pkl\"\n",
        "        with open(state_path, 'wb') as f:\n",
        "            pickle.dump(state, f)\n",
        "    \n",
        "        print(f\"Saved complete state to {state_path}\")\n",
        "        return timestamp\n",
        "\n",
        "    @classmethod\n",
        "    def load_state(cls, save_dir: str, timestamp: str, prefix: str = \"chunk\"):\n",
        "        \"\"\"Load a chatbot state from saved files\"\"\"\n",
        "        # First try to find the exact file\n",
        "        files = os.listdir(save_dir)\n",
        "        matching_files = [f for f in files if timestamp in f and f.startswith(prefix)]\n",
        "    \n",
        "        if not matching_files:\n",
        "            raise FileNotFoundError(f\"No checkpoint file found with timestamp {timestamp}\")\n",
        "    \n",
        "        # Use the actual filename that was found\n",
        "        actual_file = matching_files[0]\n",
        "        state_path = f\"{save_dir}/{actual_file}\"\n",
        "    \n",
        "        print(f\"Loading state from {state_path}\")\n",
        "    \n",
        "        with open(state_path, 'rb') as f:\n",
        "            state = pickle.load(f)\n",
        "    \n",
        "        # Create new chatbot instance\n",
        "        chatbot = cls(qa_model=state['model_name'])\n",
        "    \n",
        "        # Restore context selector state\n",
        "        chatbot.context_selector.contexts = state['context_selector']['contexts']\n",
        "        chatbot.context_selector.embeddings = state['context_selector']['embeddings']\n",
        "        chatbot.context_selector.window_size = state['context_selector']['window_size']\n",
        "        chatbot.context_selector.stride = state['context_selector']['stride']\n",
        "        chatbot.context_selector.batch_size = state['context_selector']['batch_size']\n",
        "    \n",
        "        print(f\"After loading: {len(chatbot.context_selector.contexts)} contexts and embeddings shape {chatbot.context_selector.embeddings.shape}\")\n",
        "    \n",
        "        return chatbot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_latest_timestamp(save_dir):\n",
        "    \"\"\"Simplified timestamp retrieval\"\"\"\n",
        "    # Look for regular chunk files (excluding error files)\n",
        "    files = os.listdir(save_dir)\n",
        "    chunk_files = [f for f in files if f.startswith('chunk_') and not f.startswith('error')]\n",
        "    \n",
        "    if not chunk_files:\n",
        "        return None\n",
        "    \n",
        "    # Get the latest file (based on timestamp in filename)\n",
        "    latest_file = max(chunk_files)\n",
        "    # Extract timestamp from the filename (everything after the second underscore)\n",
        "    timestamp = latest_file.split('_', 2)[2].replace('.pkl', '')\n",
        "    \n",
        "    print(f\"Found latest checkpoint: {latest_file}\")  # Debug info\n",
        "    return timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_large_csv(file_path, save_dir='processed_chunks', window_size=500,\n",
        "                     stride=250, batch_size=16, chunk_size=10000, start_chunk=0):\n",
        "    \n",
        "    \"\"\"\n",
        "    Process large CSV file with automatic saving and ability to resume.\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file\n",
        "        save_dir (str): Directory to save processed chunks\n",
        "        window_size (int): Window size for processing\n",
        "        stride (int): Stride size for processing\n",
        "        batch_size (int): Batch size for processing\n",
        "        chunk_size (int): Number of rows to process at once\n",
        "        start_chunk (int): Chunk number to start/resume from\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Initialize or load chatbot\n",
        "    if start_chunk > 0: # If resuming from a later chunk\n",
        "        latest_timestamp = get_latest_timestamp(save_dir)\n",
        "        if latest_timestamp:\n",
        "            print(f\"Resuming from checkpoint with timestamp: {latest_timestamp}\")\n",
        "            chatbot = DRChatbot.load_state(save_dir, latest_timestamp)\n",
        "            print(f\"Loaded chatbot with {len(chatbot.context_selector.contexts)} existing contexts\")\n",
        "            print(f\"Embeddings shape: {chatbot.context_selector.embeddings.shape}\")\n",
        "        else:\n",
        "            print(\"No checkpoint found, starting from beginning\")\n",
        "            chatbot = DRChatbot(window_size=window_size, stride=stride, batch_size=batch_size)\n",
        "\n",
        "    else: # If starting from the beginning\n",
        "        chatbot = DRChatbot(window_size=window_size, stride=stride, batch_size=batch_size)\n",
        "    \n",
        "    try:\n",
        "        # Process in chunks with periodic saving\n",
        "        for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
        "            if i < start_chunk:\n",
        "                continue\n",
        "                \n",
        "            contexts = chatbot.preprocess_data(chunk)\n",
        "            print(f\"Preprocessed chunk {i+1}, got {len(contexts)} contexts\")\n",
        "            \n",
        "            if not contexts:\n",
        "                print(f\"Warning: Chunk {i+1} produced no contexts, skipping\")\n",
        "                continue\n",
        "\n",
        "            if i == 0:\n",
        "                print(\"Fitting initial contexts\")\n",
        "                chatbot.fit_context_selector(contexts)\n",
        "            else:\n",
        "                print(f\"Updating with chunk {i+1}\") # Debug info\n",
        "                chatbot.update_context_selector(contexts)\n",
        "            \n",
        "            # Save every N chunks or on the last chunk\n",
        "            if (i + 1) % 5 == 0:  # Save every 5 chunks\n",
        "                chatbot.save_state(save_dir, f\"chunk_{i}\")\n",
        "                gc.collect()  # Clean up memory\n",
        "                \n",
        "            print(f\"Processed chunk {i+1}, total contexts: {len(chatbot.context_selector.contexts)}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        # Save on error\n",
        "        error_timestamp = chatbot.save_state(save_dir, f\"error_chunk_{i}\")\n",
        "        print(f\"Error at chunk {i+1}: {str(e)}\")\n",
        "        print(f\"Progress saved. To resume, use start_chunk={i}\")\n",
        "        raise\n",
        "        \n",
        "    # Final save\n",
        "    chatbot.save_state(save_dir, \"final\")\n",
        "    return chatbot\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51aDH1g2F86a",
        "outputId": "f55598da-56df-4ae8-bfb9-a2f679524979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(start_chunk = 0):\n",
        "    file_path = 'million_sample.csv'\n",
        "    save_dir = 'Pickle_data_chunks'\n",
        "    \n",
        "    try:\n",
        "        # Start or resume processing\n",
        "        chatbot = process_large_csv(\n",
        "            file_path=file_path,\n",
        "            save_dir=save_dir,\n",
        "            window_size=500,\n",
        "            stride=250,\n",
        "            batch_size=16,\n",
        "            chunk_size=10000,\n",
        "            start_chunk=start_chunk  # Change this if resuming from a specific chunk\n",
        "        )\n",
        "        \n",
        "        # Test the chatbot\n",
        "        test_questions = [\n",
        "            \"What are the symptoms of asthma?\",\n",
        "            \"How is diabetes diagnosed?\",\n",
        "            \"What are common treatments for hypertension?\"\n",
        "        ]\n",
        "        \n",
        "        for question in test_questions:\n",
        "            answer = chatbot.answer_question(question)\n",
        "            print(f\"\\nQ: {question}\")\n",
        "            print(f\"A: {answer}\")\n",
        "            \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProcessing interrupted by user. Progress has been saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred: {str(e)}\")\n",
        "        print(\"Progress has been saved and can be resumed later.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "263bf26def78401ca5c185cf0c295c33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tyzwh\\OneDrive\\AI\\bootcamp\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tyzwh\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31c8a0b6dea34b278d5e292454870fe5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0436e5bb2c3422e9c34dfd0b0b8c7a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b01a133bec546e695bdd49f2c8966c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2532556bda84764a8f2ee38d6c74e20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9bcd93dad404013811ff7f7e2c90d4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2b88488f68a44638264a2bcc13b799b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc4d5a945858451593b78c18527abc51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a289c8edae6147f8889df2f6e28f24fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a6e8b609e244563b50345b5504ceba8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b732004248b4e76b4200e5a739c45fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed chunk 1, total contexts: 19750\n",
            "Processed chunk 2, total contexts: 29750\n",
            "Processed chunk 3, total contexts: 39750\n",
            "Processed chunk 4, total contexts: 49750\n",
            "Saved complete state to Pickle_data_chunks/chunk_4_20241025_000619.pkl\n",
            "Saved complete state to Pickle_data_chunks/error_chunk_4_20241025_000620.pkl\n",
            "Error at chunk 5: name 'gc' is not defined\n",
            "Progress saved. To resume, use start_chunk=4\n",
            "\n",
            "An error occurred: name 'gc' is not defined\n",
            "Progress has been saved and can be resumed later.\n"
          ]
        }
      ],
      "source": [
        "# Run the main function\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint found, starting from beginning\n",
            "\n",
            "Processing interrupted by user. Progress has been saved.\n"
          ]
        }
      ],
      "source": [
        "main(start_chunk=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found latest checkpoint: chunk_4_20241025_000619.pkl\n",
            "Latest timestamp: 20241025_000619\n",
            "Found latest checkpoint: chunk_4_20241025_000619.pkl\n",
            "Resuming from checkpoint with timestamp: 20241025_000619\n",
            "Loading state from Pickle_data_chunks/chunk_4_20241025_000619.pkl\n",
            "After loading: 59750 contexts and embeddings shape (59750, 384)\n",
            "Loaded chatbot with 59750 existing contexts\n",
            "Embeddings shape: (59750, 384)\n",
            "Preprocessed chunk 5, got 10000 contexts\n",
            "Updating with chunk 5\n",
            "Updating with 10000 new contexts\n",
            "Before update: 59750 contexts, embeddings shape: (59750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 69750 contexts, embeddings shape: (69750, 384)\n",
            "Saved complete state to Pickle_data_chunks/chunk_4_20241025_200052.pkl\n",
            "Processed chunk 5, total contexts: 69750\n",
            "Preprocessed chunk 6, got 10000 contexts\n",
            "Updating with chunk 6\n",
            "Updating with 10000 new contexts\n",
            "Before update: 69750 contexts, embeddings shape: (69750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 79750 contexts, embeddings shape: (79750, 384)\n",
            "Processed chunk 6, total contexts: 79750\n",
            "Preprocessed chunk 7, got 10000 contexts\n",
            "Updating with chunk 7\n",
            "Updating with 10000 new contexts\n",
            "Before update: 79750 contexts, embeddings shape: (79750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 89750 contexts, embeddings shape: (89750, 384)\n",
            "Processed chunk 7, total contexts: 89750\n",
            "Preprocessed chunk 8, got 10000 contexts\n",
            "Updating with chunk 8\n",
            "Updating with 10000 new contexts\n",
            "Before update: 89750 contexts, embeddings shape: (89750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 99750 contexts, embeddings shape: (99750, 384)\n",
            "Processed chunk 8, total contexts: 99750\n",
            "Preprocessed chunk 9, got 10000 contexts\n",
            "Updating with chunk 9\n",
            "Updating with 10000 new contexts\n",
            "Before update: 99750 contexts, embeddings shape: (99750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 109750 contexts, embeddings shape: (109750, 384)\n",
            "Processed chunk 9, total contexts: 109750\n",
            "Preprocessed chunk 10, got 10000 contexts\n",
            "Updating with chunk 10\n",
            "Updating with 10000 new contexts\n",
            "Before update: 109750 contexts, embeddings shape: (109750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 119750 contexts, embeddings shape: (119750, 384)\n",
            "Saved complete state to Pickle_data_chunks/chunk_9_20241025_210355.pkl\n",
            "Processed chunk 10, total contexts: 119750\n",
            "Preprocessed chunk 11, got 10000 contexts\n",
            "Updating with chunk 11\n",
            "Updating with 10000 new contexts\n",
            "Before update: 119750 contexts, embeddings shape: (119750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 129750 contexts, embeddings shape: (129750, 384)\n",
            "Processed chunk 11, total contexts: 129750\n",
            "Preprocessed chunk 12, got 10000 contexts\n",
            "Updating with chunk 12\n",
            "Updating with 10000 new contexts\n",
            "Before update: 129750 contexts, embeddings shape: (129750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 139750 contexts, embeddings shape: (139750, 384)\n",
            "Processed chunk 12, total contexts: 139750\n",
            "Preprocessed chunk 13, got 10000 contexts\n",
            "Updating with chunk 13\n",
            "Updating with 10000 new contexts\n",
            "Before update: 139750 contexts, embeddings shape: (139750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 149750 contexts, embeddings shape: (149750, 384)\n",
            "Processed chunk 13, total contexts: 149750\n",
            "Preprocessed chunk 14, got 10000 contexts\n",
            "Updating with chunk 14\n",
            "Updating with 10000 new contexts\n",
            "Before update: 149750 contexts, embeddings shape: (149750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 159750 contexts, embeddings shape: (159750, 384)\n",
            "Processed chunk 14, total contexts: 159750\n",
            "Preprocessed chunk 15, got 10000 contexts\n",
            "Updating with chunk 15\n",
            "Updating with 10000 new contexts\n",
            "Before update: 159750 contexts, embeddings shape: (159750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 169750 contexts, embeddings shape: (169750, 384)\n",
            "Saved complete state to Pickle_data_chunks/chunk_14_20241025_215301.pkl\n",
            "Processed chunk 15, total contexts: 169750\n",
            "Preprocessed chunk 16, got 10000 contexts\n",
            "Updating with chunk 16\n",
            "Updating with 10000 new contexts\n",
            "Before update: 169750 contexts, embeddings shape: (169750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 179750 contexts, embeddings shape: (179750, 384)\n",
            "Processed chunk 16, total contexts: 179750\n",
            "Preprocessed chunk 17, got 10000 contexts\n",
            "Updating with chunk 17\n",
            "Updating with 10000 new contexts\n",
            "Before update: 179750 contexts, embeddings shape: (179750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 189750 contexts, embeddings shape: (189750, 384)\n",
            "Processed chunk 17, total contexts: 189750\n",
            "Preprocessed chunk 18, got 10000 contexts\n",
            "Updating with chunk 18\n",
            "Updating with 10000 new contexts\n",
            "Before update: 189750 contexts, embeddings shape: (189750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 199750 contexts, embeddings shape: (199750, 384)\n",
            "Processed chunk 18, total contexts: 199750\n",
            "Preprocessed chunk 19, got 10000 contexts\n",
            "Updating with chunk 19\n",
            "Updating with 10000 new contexts\n",
            "Before update: 199750 contexts, embeddings shape: (199750, 384)\n",
            "New embeddings shape: (10000, 384)\n",
            "After update: 209750 contexts, embeddings shape: (209750, 384)\n",
            "Processed chunk 19, total contexts: 209750\n",
            "Preprocessed chunk 20, got 10000 contexts\n",
            "Updating with chunk 20\n",
            "Updating with 10000 new contexts\n",
            "Before update: 209750 contexts, embeddings shape: (209750, 384)\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Test the timestamp function\n",
        "save_dir = 'Pickle_data_chunks'\n",
        "timestamp = get_latest_timestamp(save_dir)\n",
        "print(f\"Latest timestamp: {timestamp}\")\n",
        "\n",
        "# Then run the main process\n",
        "main(start_chunk=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

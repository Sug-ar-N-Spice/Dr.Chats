{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection...\n",
      "\n",
      "Processing query: STI\n",
      "Found 25 matching articles\n",
      "Fetching articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:18<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: HIV\n",
      "Found 25 matching articles\n",
      "Fetching articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: AIDS\n",
      "Found 25 matching articles\n",
      "Fetching articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:20<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: STD\n",
      "Found 25 matching articles\n",
      "Fetching articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: Gonorrhea\n",
      "Found 25 matching articles\n",
      "Fetching articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: Chlamydia\n",
      "Found 25 matching articles\n",
      "Fetching articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Total articles: 150\n",
      "\n",
      "Articles per query:\n",
      "search_query\n",
      "STI          25\n",
      "HIV          25\n",
      "AIDS         25\n",
      "STD          25\n",
      "Gonorrhea    25\n",
      "Chlamydia    25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample titles:\n",
      "                                               title search_query\n",
      "0  Examining concordance of sexual-related factor...          STI\n",
      "1  Halotolerant phosphate solubilizing bacteria i...          STI\n",
      "2  Methamphetamine abuse impairs sequential worki...          STI\n",
      "3  Barriers and facilitators to women’s access to...          STI\n",
      "4  8th Public Health Palliative Care Internationa...          STI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "class PMCOpenAccessCollector:\n",
    "    def __init__(self, save_dir: str = \"pmc_data\", email: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the collector with a directory to save downloaded files.\n",
    "        \"\"\"\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        self.email = \"NCBI_API_KEY\"\n",
    "\n",
    "        # API endpoints\n",
    "        self.esearch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "        self.efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "    def search_articles(self, query: str, max_results: int = 100) -> list:\n",
    "        \"\"\"\n",
    "        Search for articles matching the query in PMC.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'db': 'pmc',\n",
    "            'term': f\"{query} AND open access[filter]\",  # Only get open access articles\n",
    "            'retmax': max_results,\n",
    "            'retmode': 'xml',\n",
    "            'tool': 'PMCOpenAccessCollector',\n",
    "            'API_KEY': self.email\n",
    "        }\n",
    "\n",
    "        response = requests.get(self.esearch_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Search failed with status code: {response.status_code}\")\n",
    "\n",
    "        root = ET.fromstring(response.content)\n",
    "        id_list = root.findall('.//IdList/Id')\n",
    "        return [id_elem.text for id_elem in id_list]\n",
    "\n",
    "    def fetch_article(self, pmcid: str) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch and parse a single article by PMC ID.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'db': 'pmc',\n",
    "            'id': pmcid,\n",
    "            'retmode': 'xml',\n",
    "            'tool': 'PMCOpenAccessCollector',\n",
    "            'API_KEY': self.email\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.efetch_url, params=params)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch PMCID {pmcid}\")\n",
    "                return None\n",
    "\n",
    "            root = ET.fromstring(response.content)\n",
    "\n",
    "            # Extract article data\n",
    "            article_data = {\n",
    "                'pmc_id': pmcid,\n",
    "                'title': '',\n",
    "                'abstract': '',\n",
    "                'full_text': '',\n",
    "                'keywords': [],\n",
    "                'publication_date': '',\n",
    "                'journal': ''\n",
    "            }\n",
    "\n",
    "            # Extract title\n",
    "            title_elem = root.find(\".//article-title\")\n",
    "            if title_elem is not None and title_elem.text:\n",
    "                article_data['title'] = title_elem.text\n",
    "\n",
    "            # Extract abstract\n",
    "            abstract_paras = root.findall(\".//abstract//p\")\n",
    "            article_data['abstract'] = \" \".join(\n",
    "                p.text for p in abstract_paras if p is not None and p.text\n",
    "            )\n",
    "\n",
    "            # Extract main text\n",
    "            body_paras = root.findall(\".//body//p\")\n",
    "            article_data['full_text'] = \" \".join(\n",
    "                p.text for p in body_paras if p is not None and p.text\n",
    "            )\n",
    "\n",
    "            # Extract keywords\n",
    "            kwd_group = root.findall(\".//kwd-group/kwd\")\n",
    "            article_data['keywords'] = [\n",
    "                k.text for k in kwd_group if k is not None and k.text\n",
    "            ]\n",
    "\n",
    "            # Extract journal title\n",
    "            journal_elem = root.find(\".//journal-title\")\n",
    "            if journal_elem is not None and journal_elem.text:\n",
    "                article_data['journal'] = journal_elem.text\n",
    "\n",
    "            # Extract publication date\n",
    "            pub_date = root.find(\".//pub-date[@pub-type='epub']\")\n",
    "            if pub_date is not None:\n",
    "                year = pub_date.find('year')\n",
    "                month = pub_date.find('month')\n",
    "                day = pub_date.find('day')\n",
    "                date_parts = []\n",
    "                for part in [year, month, day]:\n",
    "                    if part is not None and part.text:\n",
    "                        date_parts.append(part.text)\n",
    "                article_data['publication_date'] = '-'.join(date_parts)\n",
    "\n",
    "            return article_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PMCID {pmcid}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def collect_dataset(self, queries: list, max_articles_per_query: int = 25) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collect and process articles based on specific queries.\n",
    "        \"\"\"\n",
    "        all_articles = []\n",
    "\n",
    "        for query in queries:\n",
    "            print(f\"\\nProcessing query: {query}\")\n",
    "\n",
    "            # Search for articles\n",
    "            pmcids = self.search_articles(query, max_results=max_articles_per_query)\n",
    "            print(f\"Found {len(pmcids)} matching articles\")\n",
    "\n",
    "            # Fetch each article\n",
    "            print(\"Fetching articles...\")\n",
    "            for pmcid in tqdm(pmcids):\n",
    "                article_data = self.fetch_article(pmcid)\n",
    "                if article_data:\n",
    "                    article_data['search_query'] = query\n",
    "                    all_articles.append(article_data)\n",
    "                time.sleep(0.34)  # Respect NCBI's rate limits\n",
    "\n",
    "        return pd.DataFrame(all_articles)\n",
    "\n",
    "def main():\n",
    "    collector = PMCOpenAccessCollector()\n",
    "    queries = [\n",
    "        \"STI\",\n",
    "        \"HIV\",\n",
    "        \"AIDS\",\n",
    "        \"STD\",\n",
    "        \"Gonorrhea\",\n",
    "        \"Chlamydia\",\n",
    "    ]\n",
    "\n",
    "    # Collect data\n",
    "    print(\"Starting data collection...\")\n",
    "    df = collector.collect_dataset(\n",
    "        queries=queries,\n",
    "        max_articles_per_query=25  # Adjust as needed\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = \"pmc_dataset_simon.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total articles: {len(df)}\")\n",
    "    print(\"\\nArticles per query:\")\n",
    "    print(df['search_query'].value_counts())\n",
    "    print(\"\\nSample titles:\")\n",
    "    print(df[['title', 'search_query']].head())\n",
    "\n",
    "    # Save some sample text for verification\n",
    "    with open(\"sample_articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.head().iterrows():\n",
    "            f.write(f\"Title: {row['title']}\\n\")\n",
    "            f.write(f\"Query: {row['search_query']}\\n\")\n",
    "            f.write(f\"Abstract: {row['abstract'][:500]}...\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PMCOpenAccessCollector.__init__() got an unexpected keyword argument 'emails'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 192\u001b[0m\n\u001b[1;32m    189\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 192\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[3], line 146\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(): \u001b[38;5;66;03m#why are we calling it main? can we call somthing else?\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Initialize collector\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     collector \u001b[38;5;241m=\u001b[39m PMCOpenAccessCollector(\n\u001b[1;32m    147\u001b[0m         save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpmc_oa_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    148\u001b[0m         emails\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/simonponce/Desktop/AI Bootcamp /06-Sourcing-AI-Project-Data/1/02-Ins_Pandas_Read_HTML/Solved/thekeys.env\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m    149\u001b[0m         \u001b[38;5;66;03m# Replace with your email #replace with .env variable\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Define queries #possibly list of specific STIs to get better information #testing chlamydia first\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# sexually transmitted desease as a querey results in 0 articles found\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     queries \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msexually transmitted infection\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchlamydia\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m     ]\n",
      "\u001b[0;31mTypeError\u001b[0m: PMCOpenAccessCollector.__init__() got an unexpected keyword argument 'emails'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "class PMCOpenAccessCollector:\n",
    "    def __init__(self, save_dir: str = \"pmc_data\", email: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the collector with a directory to save downloaded files.\n",
    "        \"\"\"\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        self.email = \"NCBI_API_KEY\"\n",
    "\n",
    "        # API endpoints\n",
    "        self.esearch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "        self.efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "    def search_articles(self, query: str, max_results: int = 100) -> list:\n",
    "        \"\"\"\n",
    "        Search for articles matching the query in PMC.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'db': 'pmc',\n",
    "            'term': f\"{query} AND open access[filter]\",  # Only get open access articles\n",
    "            'retmax': max_results,\n",
    "            'retmode': 'xml',\n",
    "            'tool': 'PMCOpenAccessCollector',\n",
    "            'API_KEY': self.email\n",
    "        }\n",
    "\n",
    "        response = requests.get(self.esearch_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Search failed with status code: {response.status_code}\")\n",
    "\n",
    "        root = ET.fromstring(response.content)\n",
    "        id_list = root.findall('.//IdList/Id')\n",
    "        return [id_elem.text for id_elem in id_list]\n",
    "\n",
    "    def fetch_article(self, pmcid: str) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch and parse a single article by PMC ID.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'db': 'pmc',\n",
    "            'id': pmcid,\n",
    "            'retmode': 'xml',\n",
    "            'tool': 'PMCOpenAccessCollector',\n",
    "            'API_KEY': self.email\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.efetch_url, params=params)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch PMCID {pmcid}\")\n",
    "                return None\n",
    "\n",
    "            root = ET.fromstring(response.content)\n",
    "\n",
    "            # Extract article data\n",
    "            article_data = {\n",
    "                'pmc_id': pmcid,\n",
    "                'title': '',\n",
    "                'abstract': '',\n",
    "                'full_text': '',\n",
    "                'keywords': [],\n",
    "                'publication_date': '',\n",
    "                'journal': ''\n",
    "            }\n",
    "\n",
    "            # Extract title\n",
    "            title_elem = root.find(\".//article-title\")\n",
    "            if title_elem is not None and title_elem.text:\n",
    "                article_data['title'] = title_elem.text\n",
    "\n",
    "            # Extract abstract\n",
    "            abstract_paras = root.findall(\".//abstract//p\")\n",
    "            article_data['abstract'] = \" \".join(\n",
    "                p.text for p in abstract_paras if p is not None and p.text\n",
    "            )\n",
    "\n",
    "            # Extract main text\n",
    "            body_paras = root.findall(\".//body//p\")\n",
    "            article_data['full_text'] = \" \".join(\n",
    "                p.text for p in body_paras if p is not None and p.text\n",
    "            )\n",
    "\n",
    "            # Extract keywords\n",
    "            kwd_group = root.findall(\".//kwd-group/kwd\")\n",
    "            article_data['keywords'] = [\n",
    "                k.text for k in kwd_group if k is not None and k.text\n",
    "            ]\n",
    "\n",
    "            # Extract journal title\n",
    "            journal_elem = root.find(\".//journal-title\")\n",
    "            if journal_elem is not None and journal_elem.text:\n",
    "                article_data['journal'] = journal_elem.text\n",
    "\n",
    "            # Extract publication date\n",
    "            pub_date = root.find(\".//pub-date[@pub-type='epub']\")\n",
    "            if pub_date is not None:\n",
    "                year = pub_date.find('year')\n",
    "                month = pub_date.find('month')\n",
    "                day = pub_date.find('day')\n",
    "                date_parts = []\n",
    "                for part in [year, month, day]:\n",
    "                    if part is not None and part.text:\n",
    "                        date_parts.append(part.text)\n",
    "                article_data['publication_date'] = '-'.join(date_parts)\n",
    "\n",
    "            return article_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PMCID {pmcid}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def collect_dataset(self, queries: list, max_articles_per_query: int = 25) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collect and process articles based on specific queries.\n",
    "        \"\"\"\n",
    "        all_articles = []\n",
    "\n",
    "        for query in queries:\n",
    "            print(f\"\\nProcessing query: {query}\")\n",
    "\n",
    "            # Search for articles\n",
    "            pmcids = self.search_articles(query, max_results=max_articles_per_query)\n",
    "            print(f\"Found {len(pmcids)} matching articles\")\n",
    "\n",
    "            # Fetch each article\n",
    "            print(\"Fetching articles...\")\n",
    "            for pmcid in tqdm(pmcids):\n",
    "                article_data = self.fetch_article(pmcid)\n",
    "                if article_data:\n",
    "                    article_data['search_query'] = query\n",
    "                    all_articles.append(article_data)\n",
    "                time.sleep(0.34)  # Respect NCBI's rate limits\n",
    "\n",
    "        return pd.DataFrame(all_articles)\n",
    "\n",
    "def main(): #why are we calling it main? can we call somthing else?\n",
    "    # Initialize collector\n",
    "    collector = PMCOpenAccessCollector(\n",
    "        save_dir=\"pmc_oa_data\",\n",
    "        emails=\"/Users/simonponce/Desktop/AI Bootcamp /06-Sourcing-AI-Project-Data/1/02-Ins_Pandas_Read_HTML/Solved/thekeys.env\"  \n",
    "        # Replace with your email #replace with .env variable\n",
    "    )\n",
    "\n",
    "    # Define queries #possibly list of specific STIs to get better information #testing chlamydia first\n",
    "\n",
    "    # sexually transmitted desease as a querey results in 0 articles found\n",
    "\n",
    "    queries = [\n",
    "        '\"sexually transmitted infection\"',\n",
    "        '\"chlamydia\"',\n",
    "        '\"std\"',\n",
    "        '\"sti\"',\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Collect data\n",
    "    print(\"Starting data collection...\")\n",
    "    df = collector.collect_dataset(\n",
    "        queries=queries,\n",
    "        max_articles_per_query=25  # Adjust as needed\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = \"pmc_dataset.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total articles: {len(df)}\")\n",
    "    print(\"\\nArticles per query:\")\n",
    "    print(df['search_query'].value_counts())\n",
    "    print(\"\\nSample titles:\")\n",
    "    print(df[['title', 'search_query']].head())\n",
    "\n",
    "    # Save some sample text for verification\n",
    "    with open(\"sample_articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.head().iterrows():\n",
    "            f.write(f\"Title: {row['title']}\\n\")\n",
    "            f.write(f\"Query: {row['search_query']}\\n\")\n",
    "            f.write(f\"Abstract: {row['abstract'][:500]}...\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

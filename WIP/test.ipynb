{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sug-ar-N-Spice/Dr.Chats/blob/Patricia/Patricia_Dr_chat_pre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "97Mh5ubo14ze"
      },
      "outputs": [],
      "source": [
        "# ! pip install sacremoses\n",
        "# ! pip install transformers\n",
        "# ! pip install datasets\n",
        "# ! pip install torch\n",
        "#!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4qVByYtg14zg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
        "#import gradio as gr\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from typing import List, Dict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SjtPihHI3K38"
      },
      "outputs": [],
      "source": [
        "\n",
        "class STIChatbot:\n",
        "    def __init__(self, summary_model=\"FalconsAI/medical_summarization\",\n",
        "                 qa_model=\"microsoft/BioGPT\"):\n",
        "        self.preprocessor = STIDataPreprocessor()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.summary_tokenizer = AutoTokenizer.from_pretrained(summary_model)\n",
        "        self.summary_model = AutoModelForSeq2SeqLM.from_pretrained(summary_model)\n",
        "        self.qa_tokenizer = AutoTokenizer.from_pretrained(qa_model)\n",
        "        self.qa_model = AutoModelForCausalLM.from_pretrained(qa_model)\n",
        "        self.qa_model.to(self.device)\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        return self.preprocessor.preprocess_dataframe(df)\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        inputs = self.summary_tokenizer(text, padding=True,return_tensors=\"pt\", max_length=512, truncation=True).to(self.device)\n",
        "        summary_ids = self.summary_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=150,\n",
        "            min_length=40,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        return self.summary_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def answer_question(self, context, question):\n",
        "        prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "        inputs = self.qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "        # Calculate available tokens for the answer\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        max_new_tokens = min(100, 1024 - input_length)  # Assuming 1024 is the model's maximum context length\n",
        "\n",
        "        output = self.qa_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return self.qa_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CISRX77YYsgr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oVqt8RX33K36"
      },
      "outputs": [],
      "source": [
        "\n",
        "##STOP WORDS IN NLP DONT MEAN ANYTHING LIKE WE THEY THEY JUST COMPLETE THE SENTENCE\n",
        "\n",
        "## THIS IS CLASS THAT Cleans the data related to STIs\n",
        "class STIDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessor for Sexually Transmitted Infections (STI) medical data.\n",
        "\n",
        "    This class handles cleaning, normalization, and preparation of text data\n",
        "    related to STIs for use in a medical chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the preprocessor with necessary NLTK downloads.\"\"\"\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # STI-specific terms to keep even if they're in stopwords\n",
        "        self.sti_terms = {'hiv', 'aids', 'std', 'sti', 'hpv', 'hsv'}\n",
        "        self.stop_words = self.stop_words - self.sti_terms\n",
        "\n",
        "    def clean_text(self, text: str) -> str: #str) -> str: this part is more developed skill level can do without in python but in Ctt C sharp need it\n",
        "    #\"\"\"\" is a multi-line comment\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Clean and normalize the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned and normalized text\n",
        "        \"\"\"\n",
        "\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        # text = text.lower()\n",
        "\n",
        "        # Remove special characters but keep medical symbols\n",
        "        # text = re.sub(r'[^a-zA-Z0-9\\s+\\-/%]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        # text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_stopwords(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove stopwords from the text, keeping STI-specific terms.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            str: Text with stopwords removed\n",
        "        \"\"\"\n",
        "        words = word_tokenize(text) ### this is resulting in a list of words was converting sentence / paragraph into a list of words\n",
        "\n",
        "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Preprocess the entire dataframe.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input dataframe with 'train' and 'results' columns\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Preprocessed dataframe\n",
        "        \"\"\"\n",
        "        # change columns names depending which csv file you are using cleaning text and removing stopwords\n",
        "        df['clean_abstract'] = df['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "        df['clean_full_text'] = df['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "\n",
        "        # Combine cleaned abstract and results or full_texts depending on which csv you are using\n",
        "        df['combined_text'] = df['clean_abstract'] + ' ' + df['clean_full_text']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_for_model(self, text: str, max_length: int = 512) -> str: ##looks at paragraph, cuts the paragraph if more than 512 This takes the sentence splits to words and has a max length\n",
        "        \"\"\"\n",
        "        Prepare text for model input, truncating if necessary.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_length (int): Maximum number of words\n",
        "\n",
        "        Returns:\n",
        "            str: Prepared text\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) > max_length:\n",
        "            return ' '.join(words[:max_length])\n",
        "        return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kwDrllroAuNF"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataframe(self, df): ###USUALLY YOU SEE SELF IN A CLASS This allows you to code attributes in a class\n",
        "    \"\"\"\n",
        "    Preprocess the entire dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame or DatasetDict): Input dataframe with 'abstract' and 'results' columns\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Preprocessed dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if df is a DatasetDict object-- When we try to find dataset Dict- that means we havent converted it to pandas and cleaning whole dataset\n",
        "    \n",
        "    # If it's a regular DataFrame, process as before\n",
        "    processed_df_1= df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
        "    processed_df_1['clean_abstract'] = processed_df_1['question'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "    processed_df_1['clean_full_text'] = processed_df_1['context'].apply(self.clean_text).apply(self.remove_stopwords)\n",
        "    processed_df_1['combined_text'] = processed_df_1['clean_abstract'] + ' ' + processed_df_1['clean_full_text']\n",
        "\n",
        "    return processed_df_1  # Return the processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8gMxxfua3K38",
        "outputId": "538fc62d-382d-42d2-842b-92cd1313f3ca"
      },
      "outputs": [],
      "source": [
        "# NOT NEEDED \n",
        "# \n",
        "# # Example usage\n",
        "# def main():\n",
        "#     # Load your DataFrame\n",
        "#     df_1 = pd.read_csv('million_sample.csv')  # choose a csv file\n",
        "\n",
        "#     # Initialize the preprocessor\n",
        "#     preprocessor = STIDataPreprocessor()\n",
        "\n",
        "#     # Preprocess the data\n",
        "#     processed_df = preprocessor.preprocess_dataframe(df_1)\n",
        "\n",
        "#     # Prepare a sample for model input\n",
        "#     sample_text = processed_df['combined_text'].iloc[0]\n",
        "#     model_ready_text = preprocessor.prepare_for_model(sample_text)\n",
        "\n",
        "#     print(\"Sample preprocessed and model-ready text:\")\n",
        "#     print(model_ready_text[:200] + \"...\")  # Print first 200 characters\n",
        "\n",
        "# ### if you are calling a script directly from a command prompt it runs main if not the main scrip then it doesnt run \n",
        "# ## main you dont really need this \n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNmKCSHMUsBR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_L04WrAMYOVq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sacremoses in c:\\users\\jerom\\appdata\\roaming\\python\\python312\\site-packages (0.1.1)\n",
            "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses) (2023.10.3)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses) (4.66.4)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->sacremoses) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio\n",
        "!pip install sacremoses\n",
        "import gradio as gr\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5cWNvXUkYg1i"
      },
      "outputs": [],
      "source": [
        "# # Create a function that takes in a message.\n",
        "# def run(msg):\n",
        "#     return f'Returning this message: {msg}'\n",
        "\n",
        "\n",
        "# # Create an instance of the Gradio Interface application function with the following parameters.\n",
        "# app = gr.Interface(fn=run, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "# # Launch the app\n",
        "# app.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "agCNT7fA3K38",
        "outputId": "f259a21b-ab8f-42b5-9a61-5eb7f0d93206"
      },
      "outputs": [],
      "source": [
        "# df_1 = pd.read_csv('ai_medical_dataset.csv')\n",
        "# chatbot = STIChatbot()\n",
        "# processed_df_1 = chatbot.preprocess_data(df_1)\n",
        "\n",
        "\n",
        "# dataset = load_dataset(\"ruslanmv/ai-medical-dataset\")\n",
        "# dataset.set_format(type='pandas')\n",
        "# df_1 = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# # processed_df_1 = chatbot.preprocess_data(first_10_rows_df)\n",
        "\n",
        "# # Example usage\n",
        "# # context_1 = processed_df_1['combined_text'].iloc[0]\n",
        "# context_2 = processed_df_1['combined_text']\n",
        "# summary = chatbot.generate_summary(\"context\")\n",
        "# question = \"What are the main symptoms of gonorrhea?\"\n",
        "# answer = chatbot.answer_question(summary, question)\n",
        "# print(f\"Q: {question}\\nA: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msK-ZxGrYNoe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#DID this to find the error issue \n",
        "# ERROR\n",
        "\n",
        "# An error occurred while processing your request: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
        "# chatbot = STIChatbot()\n",
        "# processed_df_1 = chatbot.preprocess_data(df_1)\n",
        "# context_2 = processed_df_1['combined_text'][:2] #.iloc[0]\n",
        "# summary = chatbot.generate_summary(list(context_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_1' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the chatbot and preprocess the data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m STIChatbot()\n\u001b[1;32m----> 5\u001b[0m processed_df_1 \u001b[38;5;241m=\u001b[39m chatbot\u001b[38;5;241m.\u001b[39mpreprocess_data(df_1)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'df_1' is not defined"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Initialize the chatbot and preprocess the data\n",
        "# processed_df_1 = chatbot.preprocess_data(df_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "preprocessed_df = pd.read_csv('processed_df_1.csv')\n",
        "chatbot = STIChatbot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0ijIHx3rfxTS"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'gr' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while processing your request: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create a Gradio interface\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m iface \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[0;32m     16\u001b[0m     fn\u001b[38;5;241m=\u001b[39mgenerate_response,            \u001b[38;5;66;03m# The function to call\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour Question\u001b[39m\u001b[38;5;124m\"\u001b[39m, placeholder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk about symptoms...\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Input type\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m),        \u001b[38;5;66;03m# Output type\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymptoms Q&A\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Title of the interface\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk about the main symptoms you are concerned with.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Launch the Gradio app\u001b[39;00m\n\u001b[0;32m     24\u001b[0m iface\u001b[38;5;241m.\u001b[39mlaunch(share\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'gr' is not defined"
          ]
        }
      ],
      "source": [
        "# Function to handle the summary and answering the question\n",
        "def generate_response(question):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a valid question.\"\n",
        "\n",
        "    try:\n",
        "        context_2 = processed_df_1['combined_text']#.iloc[0]\n",
        "        summary = chatbot.generate_summary(list(context_2))\n",
        "        answer = chatbot.answer_question(summary, question)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while processing your request: {str(e)}\"\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_response,            # The function to call\n",
        "    inputs=gr.Textbox(label=\"Your Question\", placeholder=\"Ask about symptoms...\"),  # Input type\n",
        "    outputs=gr.Textbox(label=\"Answer\"),        # Output type\n",
        "    title=\"Symptoms Q&A\",  # Title of the interface\n",
        "    description=\"Ask about the main symptoms you are concerned with.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(share=False)  # Set share=True if you want a public link\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'gradio' has no attribute 'Request'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iface \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[0;32m      2\u001b[0m     fn\u001b[38;5;241m=\u001b[39mgenerate_response,            \u001b[38;5;66;03m# The function to call\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour Question\u001b[39m\u001b[38;5;124m\"\u001b[39m, placeholder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk about symptoms...\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Input type\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m),        \u001b[38;5;66;03m# Output type\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymptoms Q&A\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Title of the interface\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk about the main symptoms you are concerned with.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Launch the Gradio app\u001b[39;00m\n\u001b[0;32m     10\u001b[0m iface\u001b[38;5;241m.\u001b[39mlaunch(share\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\interface.py:535\u001b[0m, in \u001b[0;36mInterface.__init__\u001b[1;34m(self, fn, inputs, outputs, examples, cache_examples, cache_mode, examples_per_page, example_labels, live, title, description, article, theme, flagging_mode, flagging_options, flagging_dir, flagging_callback, analytics_enabled, batch, max_batch_size, api_name, _api_mode, allow_duplication, concurrency_limit, css, css_paths, js, head, head_paths, additional_inputs, additional_inputs_accordion, submit_btn, stop_btn, clear_btn, delete_cache, show_progress, fill_width, allow_flagging, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duplicate_btn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    533\u001b[0m     duplicate_btn\u001b[38;5;241m.\u001b[39mactivate()\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattach_flagging_events(flag_btns, _clear_btn, _submit_event)\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_examples()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_article()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\interface.py:862\u001b[0m, in \u001b[0;36mInterface.attach_flagging_events\u001b[1;34m(self, flag_btns, _clear_btn, _submit_event)\u001b[0m\n\u001b[0;32m    852\u001b[0m flag_method \u001b[38;5;241m=\u001b[39m FlagMethod(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflagging_callback, label, value)\n\u001b[0;32m    853\u001b[0m flag_btn\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m    854\u001b[0m     utils\u001b[38;5;241m.\u001b[39masync_lambda(\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: Button(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving...\u001b[39m\u001b[38;5;124m\"\u001b[39m, interactive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    860\u001b[0m     show_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    861\u001b[0m )\n\u001b[1;32m--> 862\u001b[0m flag_btn\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m    863\u001b[0m     flag_method,\n\u001b[0;32m    864\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mflag_components,\n\u001b[0;32m    865\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mflag_btn,\n\u001b[0;32m    866\u001b[0m     preprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    867\u001b[0m     queue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    868\u001b[0m     show_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    869\u001b[0m )\n\u001b[0;32m    870\u001b[0m _clear_btn\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m    871\u001b[0m     utils\u001b[38;5;241m.\u001b[39masync_lambda(flag_method\u001b[38;5;241m.\u001b[39mreset),\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    875\u001b[0m     show_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    876\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\events.py:578\u001b[0m, in \u001b[0;36mEventListener._setup.<locals>.event_trigger\u001b[1;34m(block, fn, inputs, outputs, api_name, scroll_to_output, show_progress, queue, batch, max_batch_size, preprocess, postprocess, cancels, trigger_mode, js, concurrency_limit, concurrency_id, show_api, time_limit, stream_every, like_user_message)\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_event_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m outside of a gradio.Blocks context.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m     )\n\u001b[0;32m    574\u001b[0m event_target \u001b[38;5;241m=\u001b[39m EventListenerMethod(\n\u001b[0;32m    575\u001b[0m     block \u001b[38;5;28;01mif\u001b[39;00m _has_trigger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, _event_name\n\u001b[0;32m    576\u001b[0m )\n\u001b[1;32m--> 578\u001b[0m dep, dep_index \u001b[38;5;241m=\u001b[39m root_block\u001b[38;5;241m.\u001b[39mset_event_trigger(\n\u001b[0;32m    579\u001b[0m     [event_target],\n\u001b[0;32m    580\u001b[0m     fn,\n\u001b[0;32m    581\u001b[0m     inputs,\n\u001b[0;32m    582\u001b[0m     outputs,\n\u001b[0;32m    583\u001b[0m     preprocess\u001b[38;5;241m=\u001b[39mpreprocess,\n\u001b[0;32m    584\u001b[0m     postprocess\u001b[38;5;241m=\u001b[39mpostprocess,\n\u001b[0;32m    585\u001b[0m     scroll_to_output\u001b[38;5;241m=\u001b[39mscroll_to_output,\n\u001b[0;32m    586\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    587\u001b[0m     api_name\u001b[38;5;241m=\u001b[39mapi_name,\n\u001b[0;32m    588\u001b[0m     js\u001b[38;5;241m=\u001b[39mjs,\n\u001b[0;32m    589\u001b[0m     concurrency_limit\u001b[38;5;241m=\u001b[39mconcurrency_limit,\n\u001b[0;32m    590\u001b[0m     concurrency_id\u001b[38;5;241m=\u001b[39mconcurrency_id,\n\u001b[0;32m    591\u001b[0m     queue\u001b[38;5;241m=\u001b[39mqueue,\n\u001b[0;32m    592\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch,\n\u001b[0;32m    593\u001b[0m     max_batch_size\u001b[38;5;241m=\u001b[39mmax_batch_size,\n\u001b[0;32m    594\u001b[0m     trigger_after\u001b[38;5;241m=\u001b[39m_trigger_after,\n\u001b[0;32m    595\u001b[0m     trigger_only_on_success\u001b[38;5;241m=\u001b[39m_trigger_only_on_success,\n\u001b[0;32m    596\u001b[0m     trigger_mode\u001b[38;5;241m=\u001b[39mtrigger_mode,\n\u001b[0;32m    597\u001b[0m     show_api\u001b[38;5;241m=\u001b[39mshow_api,\n\u001b[0;32m    598\u001b[0m     connection\u001b[38;5;241m=\u001b[39m_connection,\n\u001b[0;32m    599\u001b[0m     time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[0;32m    600\u001b[0m     stream_every\u001b[38;5;241m=\u001b[39mstream_every,\n\u001b[0;32m    601\u001b[0m     like_user_message\u001b[38;5;241m=\u001b[39mlike_user_message,\n\u001b[0;32m    602\u001b[0m     event_specific_args\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    603\u001b[0m         d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m _event_specific_args\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m d\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent_prop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m     ]\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _event_specific_args\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    609\u001b[0m )\n\u001b[0;32m    610\u001b[0m set_cancel_events(\n\u001b[0;32m    611\u001b[0m     [event_target],\n\u001b[0;32m    612\u001b[0m     cancels,\n\u001b[0;32m    613\u001b[0m )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _callback:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\blocks.py:786\u001b[0m, in \u001b[0;36mBlocksConfig.set_event_trigger\u001b[1;34m(self, targets, fn, inputs, outputs, preprocess, postprocess, scroll_to_output, show_progress, api_name, js, no_target, queue, batch, max_batch_size, cancels, collects_event_data, trigger_after, trigger_only_on_success, trigger_mode, concurrency_limit, concurrency_id, show_api, renderable, is_cancel_function, connection, time_limit, stream_every, like_user_message, event_specific_args)\u001b[0m\n\u001b[0;32m    783\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [outputs]\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cancels:\n\u001b[1;32m--> 786\u001b[0m     check_function_inputs_match(fn, inputs, inputs_as_dict)\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _targets[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_up\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m trigger_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     trigger_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malways_last\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\utils.py:963\u001b[0m, in \u001b[0;36mcheck_function_inputs_match\u001b[1;34m(fn, inputs, inputs_as_dict)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 963\u001b[0m parameter_types \u001b[38;5;241m=\u001b[39m get_type_hints(fn)\n\u001b[0;32m    964\u001b[0m min_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    965\u001b[0m max_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\utils.py:930\u001b[0m, in \u001b[0;36mget_type_hints\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m--> 930\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typing\u001b[38;5;241m.\u001b[39mget_type_hints(fn)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\typing.py:2301\u001b[0m, in \u001b[0;36mget_type_hints\u001b[1;34m(obj, globalns, localns, include_extras)\u001b[0m\n\u001b[0;32m   2293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2294\u001b[0m         \u001b[38;5;66;03m# class-level forward refs were handled above, this must be either\u001b[39;00m\n\u001b[0;32m   2295\u001b[0m         \u001b[38;5;66;03m# a module-level annotation or a function argument annotation\u001b[39;00m\n\u001b[0;32m   2296\u001b[0m         value \u001b[38;5;241m=\u001b[39m ForwardRef(\n\u001b[0;32m   2297\u001b[0m             value,\n\u001b[0;32m   2298\u001b[0m             is_argument\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mModuleType),\n\u001b[0;32m   2299\u001b[0m             is_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2300\u001b[0m         )\n\u001b[1;32m-> 2301\u001b[0m     hints[name] \u001b[38;5;241m=\u001b[39m _eval_type(value, globalns, localns, type_params)\n\u001b[0;32m   2302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hints \u001b[38;5;28;01mif\u001b[39;00m include_extras \u001b[38;5;28;01melse\u001b[39;00m {k: _strip_annotations(t) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m hints\u001b[38;5;241m.\u001b[39mitems()}\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\typing.py:415\u001b[0m, in \u001b[0;36m_eval_type\u001b[1;34m(t, globalns, localns, type_params, recursive_guard)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate all forward references in the given type t.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03mFor use of globalns and localns see the docstring for get_type_hints().\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;124;03mrecursive_guard is used to prevent infinite recursion with a recursive\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03mForwardRef.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, ForwardRef):\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_evaluate(globalns, localns, type_params, recursive_guard\u001b[38;5;241m=\u001b[39mrecursive_guard)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (_GenericAlias, GenericAlias, types\u001b[38;5;241m.\u001b[39mUnionType)):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, GenericAlias):\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\typing.py:938\u001b[0m, in \u001b[0;36mForwardRef._evaluate\u001b[1;34m(self, globalns, localns, type_params, recursive_guard)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     locals_to_pass \u001b[38;5;241m=\u001b[39m localns\n\u001b[0;32m    937\u001b[0m type_ \u001b[38;5;241m=\u001b[39m _type_check(\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28meval\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward_code__, globalns, locals_to_pass),\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward references must evaluate to types.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    940\u001b[0m     is_argument\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward_is_argument__,\n\u001b[0;32m    941\u001b[0m     allow_special_forms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward_is_class__,\n\u001b[0;32m    942\u001b[0m )\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward_value__ \u001b[38;5;241m=\u001b[39m _eval_type(\n\u001b[0;32m    944\u001b[0m     type_,\n\u001b[0;32m    945\u001b[0m     globalns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m     recursive_guard\u001b[38;5;241m=\u001b[39m(recursive_guard \u001b[38;5;241m|\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward_arg__}),\n\u001b[0;32m    949\u001b[0m )\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward_evaluated__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32m<string>:1\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'gradio' has no attribute 'Request'"
          ]
        }
      ],
      "source": [
        "iface = gr.Interface(\n",
        "    fn=generate_response,            # The function to call\n",
        "    inputs=gr.Textbox(label=\"Your Question\", placeholder=\"Ask about symptoms...\"),  # Input type\n",
        "    outputs=gr.Textbox(label=\"Answer\"),        # Output type\n",
        "    title=\"Symptoms Q&A\",  # Title of the interface\n",
        "    description=\"Ask about the main symptoms you are concerned with.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(share=True)  # Set share=True if you want a public link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2XPZINfw-M"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZznH2nileXG7"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
